{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating a Database of Company Filings from EDGAR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import os\n",
    "import pandas as pd\n",
    "import random\n",
    "import re\n",
    "import requests\n",
    "import string\n",
    "import time\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "from io import StringIO\n",
    "\n",
    "COMPANY_REGEX = re.compile(r'^company\\.idx$')\n",
    "QUARTER_REGEX = re.compile(r'^QTR[1-4]$')\n",
    "YEAR_REGEX = re.compile(r'^\\d{4}$')\n",
    "EDGAR_BASE_URL = \"https://www.sec.gov/Archives\"  # all .idx urls are relative to this\n",
    "EDGAR_INDEX_URL = \"https://www.sec.gov/Archives/edgar/full-index/\"\n",
    "\n",
    "DOWNLOAD_PATH = \"/home/mallinger/Coding/notebooks/data/edgar\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scrape the Index Locations\n",
    "\n",
    "The EDGAR indexes are behind two layers of web pages.  The first (https://www.sec.gov/Archives/edgar/full-index/) provides a table of contents for each year.  The second (1997 example: https://www.sec.gov/Archives/edgar/full-index//1997/) provides a table of contents for the quarters in each year.\n",
    "\n",
    "Inside the quarter directories are many files.  But the .idx files are all the same, simply ordered by different criteria.  So we'll take \"company.idx\" to get the index for each quarter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_index_page(url, link_re, base_url=None):\n",
    "    \"\"\"Given a URL to a contents page, returns all the links that match\n",
    "    the provided regular expression.  Helpful for scraping table of\n",
    "    contents pages.\n",
    "    \"\"\"\n",
    "    index = requests.get(url)\n",
    "    soup = BeautifulSoup(index.content, 'html.parser')\n",
    "    # all content links on the EDGAR pages are inside the main area\n",
    "    # designated by this HTML id attribute.\n",
    "    content_links = soup.find(id=\"main-content\").find_all('a')\n",
    "    urls = []\n",
    "    for link in content_links:\n",
    "        if link_re.match(link.getText()):\n",
    "            href = link.get(\"href\")\n",
    "            if base_url:\n",
    "                href = \"{}/{}\".format(base_url, href)\n",
    "            urls.append(href)\n",
    "    return urls\n",
    "\n",
    "def parse_all_index_pages(urls, link_re):\n",
    "    \"\"\"Given a list of URLs to a contents pages, returns a list of\n",
    "    all the links found that match the link regular expression.\n",
    "    \"\"\"\n",
    "    found_urls = []\n",
    "    for url in urls:\n",
    "        found_urls.extend(parse_index_page(url, link_re, base_url=url))\n",
    "    return found_urls\n",
    "\n",
    "# get all the URLs to year partitions\n",
    "year_urls = parse_index_page(EDGAR_INDEX_URL, YEAR_REGEX, base_url=EDGAR_INDEX_URL)\n",
    "# get all the URLs to the quarter partitions\n",
    "quarter_urls = parse_all_index_pages(year_urls, QUARTER_REGEX)\n",
    "# get the company.idx file in the quarterly folder\n",
    "index_urls = parse_all_index_pages(quarter_urls, COMPANY_REGEX)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the Index\n",
    "\n",
    "Note that this code is extremely memory intensive as the number of records is in the 10's of millions.  If you don't have over 10gb of memory free, you may want to edit it into batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_index_file(url):\n",
    "    \"\"\"Given a URL to an .idx Edgar file, reads the index and returns\n",
    "    a Pandas dataframe of the results.\n",
    "    \"\"\"\n",
    "    # note: work with bytes strings here as the offsets for fixed width columns\n",
    "    # get thrown off if there are conversion errors or bad characters (There are!)\n",
    "    rows = requests.get(url).content.split(b\"\\n\")\n",
    "    # there is a header consisting of leading rows with comments followed by\n",
    "    # a row of hyphens to start the data (------)\n",
    "    row_offset = 1 + min([i for i, row in enumerate(rows) if re.match(b\"------\", row)])\n",
    "    rows = rows[row_offset:]\n",
    "    # skip any empty rows\n",
    "    rows = list(filter(lambda row: not re.match(b\"^\\s*$\", row), rows))\n",
    "        \n",
    "    # note, we can't use pd.read_fwf because of invalid characters in the bytes records.\n",
    "    # this approach avoids lost records\n",
    "    array_frame = [(row[0:62].strip().decode(\"utf-8\", \"ignore\"),\n",
    "                    row[62:74].strip().decode(\"utf-8\", \"ignore\"),\n",
    "                    row[74:86].strip().decode(\"utf-8\", \"ignore\"),\n",
    "                    row[86:98].strip().decode(\"utf-8\", \"ignore\"),\n",
    "                    row[98:].strip().decode(\"utf-8\", \"ignore\")) \n",
    "                   for row in rows]\n",
    "    df = pd.DataFrame(array_frame, columns=[\"name\", \"form\", \"cik\", \"date\", \"url\"])\n",
    "    df[\"date\"] = pd.to_datetime(df[\"date\"])\n",
    "    return df\n",
    "\n",
    "index = None\n",
    "for url in index_urls:\n",
    "    df = parse_index_file(url)\n",
    "    if index is None:\n",
    "        index = df\n",
    "    else:\n",
    "        index = pd.concat([index, df], ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index.to_pickle(os.path.join(DOWNLOAD_PATH, \"edgar_index.pickle.gzip\"), compression=\"gzip\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the Index\n",
    "\n",
    "(This cell for ease of starting with saved data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = pd.read_pickle(os.path.join(DOWNLOAD_PATH, \"edgar_index.pickle.gzip\"), compression=\"gzip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_form_424b2 = index[\"form\"].str.contains(\"424B2\")\n",
    "\n",
    "# create a dataset to download\n",
    "records = index[is_form_424b2]\n",
    "counter = 0\n",
    "for _, record in records.iterrows():\n",
    "    url = \"{}/{}\".format(EDGAR_BASE_URL, record[\"url\"])\n",
    "    content = requests.get(url).content.decode(\"utf-8\", \"ignore\")\n",
    "    # add a random padding to avoid filename colisions\n",
    "    padding = ''.join([random.choice(string.ascii_letters + string.digits) for i in range(5)])\n",
    "    filename = \"424b-{cik}-{date}-{padding}.txt\".format(\n",
    "                    cik=record[\"cik\"],\n",
    "                    date=record[\"date\"].strftime(\"%Y%m%d\"),\n",
    "                    padding=padding)\n",
    "    \n",
    "    with open(os.path.join(DOWNLOAD_PATH, filename), \"w\") as fh:\n",
    "        fh.write(content)\n",
    "        \n",
    "    # sleep for a second every 10 records\n",
    "    counter += 1\n",
    "    if counter % 10 == 0:\n",
    "        time.sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# content = b\"\\n\".join(rows[row_offset:])\n",
    "# return pd.read_fwf(StringIO(content), \n",
    "#                    colspecs=[(0,62), (62,74), (74,86), (86,98), (98, 151)], \n",
    "#                    names=[\"company\", \"form\", \"cik\", \"date\", \"url\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
