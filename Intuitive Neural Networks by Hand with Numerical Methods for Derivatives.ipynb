{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intuitive Neural Networks by Hand\n",
    "\n",
    "This notebook shows how to build a neural network by hand.  The motivation for creating _yet another_ neural network from scratch post is that I believe most get lost in the calculus and lose the concepts.  Additionally, I believe the demonstation code is often designed to be efficient with numpy, not illustrative.\n",
    "\n",
    "Below, we create classes to represent our neural network and then use it to fit to a simple example from the Iris dataset.\n",
    "\n",
    "* NOTE: Code is still under development and bugs likely exist."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Concepts\n",
    "\n",
    "Neural networks are essentially two things:\n",
    "1. A collection of node layers with connecting weights\n",
    "1. A forwards-backwards algorithm (of sorts) for optimizing those weights.\n",
    "\n",
    "The forward-backward algorithm is a common paradigm outside of neural networks and is helpful to learn about. By analogy, the forward step simply computes the current outcome probabilities.  The backwards step updates the weights used in making predictions to those that seem best at the current iteration.  This is the \"gradient descent\" that is frequently mentioned (and used).  The bidirectional process continues (hopefully) to convergence.\n",
    "\n",
    "At prediction time, one is essentially re-computing the forward step of the neural network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"static/neural-network.png\" alt=\"drawing\" width=\"500\"/>\n",
    "\n",
    "A key concept is that neural network weights are linear, with each node receiving a linear combination of the node outputs from the prior layer. For the first layer, nodes receive a linear combination of the dataset features.  For subsequent layers, nodes receive a linear combination of the activation functions of the previous nodes.  In this way, neural networks continually augment the data with linear combinations (followed by non-linear activation functions), projecting iterations of data into new dimensions until a good prediction is made.  As layers get arbitrarily large, one can imagine how these projections capture latent features.\n",
    "\n",
    "Now notice that the weights between these layers are simply edges on a bipartite graph.  This gives us our insight that we can represent the weights as a matrix between the current layer and the prior layer:\n",
    "\n",
    "<img src=\"static/bipartite-graph.png\" alt=\"drawing\" width=\"500\"/>\n",
    "\n",
    "Thus it follows that weighting the input is simply a matrix multiplication by the adjacency matrix: _W*input + b_."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Layer (Incomplete Version)\n",
    "\n",
    "The first thing that our neural network will need is a notion of a layer.  As with any coding project, we do not know our final state API *yet*, but we have a notion of how to start.  Namely, our Layer will need a size (number of nodes), an activation function, weights, and betas.  We also use a locator pattern to store to which model iteration this Layer belongs (note that we have not defined model iterations yet!).\n",
    "\n",
    "The IncompleteLayer class knows how to apply weights using our bipartite graph insight.  We simply multiply the weights by the input matrix and add our beta terms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class IncompleteLayer:\n",
    "    def __init__(self, model_iteration, size, activation_function, weights, betas):\n",
    "        self.model_iteration = model_iteration  # locator pattern\n",
    "        self.size = size\n",
    "        self.activation_function = activation_function\n",
    "        self.weights = weights\n",
    "        self.betas = betas\n",
    "\n",
    "    def apply_weights(self, layer_input):\n",
    "        \"\"\"Equivalent to a forward propegation on a single layer\"\"\"\n",
    "        Z = np.dot(self.weights, layer_input) + self.betas\n",
    "        output = self.activation_function(Z)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Iteration\n",
    "\n",
    "Every neural network model runs for multiple iterations and these iterations have their own layers, weights, etc.  Thus it makes for a good class to build.\n",
    "\n",
    "Our ModelIteration class should have a `feed_forward` function that runs our `Layer.apply_weights` and then returns the last value (the output of the last layer is the neural network output).  Similarly, predict is simply a call to `feed_forward` under this methodology.\n",
    "\n",
    "We will also need a way to do backprogegation.  The derivative calculations can be passed to Layers.  (Note that we have implicitly defined `Layer.update_weights`.)\n",
    "\n",
    "### Setting Weights\n",
    "\n",
    "Despite defining much of the model iterations and layers, we never wrote any code that sets the values of weights. \n",
    "\n",
    "**On any forward feeding of a model, the weights are defined to be either random initialization or the weights from the prior backpropegation.** Thus ModelIteration will need to loop to create the layers, each time looking at the prior iteration values:\n",
    "\n",
    "```\n",
    "for layer in model_structure:\n",
    "    if prior_iteration:\n",
    "        weights = prior_iteration.weights\n",
    "        betas = prior_iteration.betas\n",
    "    else:\n",
    "        weights = random_initialization()\n",
    "        betas = zeros()\n",
    "```\n",
    "\n",
    "A final wrinkle: To initialize the weights on the first iteration, we must be able to define the dimensions of the weight matrix.  From our bipartite graph, the matrix dimensions will be *(# current layer nodes, # prior layer nodes)*.  However, remember that the first layer has *# dataset features* columns as there is no prior layer.  The `__init__` method of the ModelIteration class updates the pseudocode to include these dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class ModelIteration:\n",
    "    def __init__(self, model, data, Y, learning_rate, prior_iteration=None):\n",
    "        self.model = model  # locator pattern\n",
    "        self.learning_rate = learning_rate\n",
    "        self.prior_iteration = prior_iteration\n",
    "        self.layers = []\n",
    "        \n",
    "        # iterate over each layer to initialize weights, per the above description.\n",
    "        for layer_number, layer in enumerate(self.model.model_structure):\n",
    "            if self.prior_iteration is None: # first iteration, must initialize weights\n",
    "                if 0 == layer_number:\n",
    "                    prior_layer_size = data.shape[1]\n",
    "                else:\n",
    "                    prior_layer_size = self.layers[-1].size\n",
    "                weights = np.random.randn(layer[\"size\"], prior_layer_size)\n",
    "                betas = np.zeros((layer[\"size\"], 1))\n",
    "            else:\n",
    "                weights = self.prior_iteration.layers[layer_number].weights # backprop output\n",
    "                betas = self.prior_iteration.layers[layer_number].betas            \n",
    "           \n",
    "            layer = Layer(self, layer[\"size\"], layer[\"activation\"], weights, betas)\n",
    "            self.layers.append(layer)\n",
    "        \n",
    "    def feed_forward(self, data):\n",
    "        prior_output = self.model.data.T\n",
    "        for layer in self.layers:\n",
    "            output = layer.apply_weights(prior_output)\n",
    "            prior_output = output\n",
    "        return output\n",
    "    \n",
    "    def predict(self, data):\n",
    "         return self.feed_forward(data)\n",
    "        \n",
    "    def evaluate(self, data=None, Y=None):\n",
    "        if data is None:\n",
    "            data = self.model.data\n",
    "            Y = self.model.Y\n",
    "        predictions = self.predict(data)\n",
    "        return self.model.cost_function(predictions, Y)\n",
    "\n",
    "    def propegate_backward(self):\n",
    "        derivatives_list = []\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            derivatives = layer.calculate_derivatives()\n",
    "            derivatives_list.append(derivatives)\n",
    "            \n",
    "        for i, layer in enumerate(self.layers):\n",
    "            layer.update_derivatives(derivatives_list[i])\n",
    "            layer.update_weights(self.learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Class\n",
    "\n",
    "Our model class is effectively a wrapper for ModelIteration that calls the last iteration for `predict`.  The only difference is that our Model class needs a `train` method.  `train` will loop over model iterations, feed forward and back propegate, printing status along the way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class Model:\n",
    "    def __init__(self, data, Y, model_structure, cost_function, learning_rate):\n",
    "        self.data = data\n",
    "        self.Y = Y\n",
    "        self.model_structure = model_structure\n",
    "        self.cost_function = cost_function\n",
    "        self.learning_rate = learning_rate\n",
    "        self.iterations = None\n",
    "        \n",
    "    def train(self, learning_rate=0.01, num_iterations=5000):\n",
    "        self.iterations = []\n",
    "        prior_iteration = None\n",
    "        for iteration in range(num_iterations):\n",
    "            model_iteration = ModelIteration(self, self.data, self.Y, learning_rate, prior_iteration)\n",
    "            self.iterations.append(model_iteration)\n",
    "            \n",
    "            iteration_output = model_iteration.feed_forward(self.data)\n",
    "            model_iteration.propegate_backward() # update weights\n",
    "    \n",
    "            prior_iteration = model_iteration\n",
    "            \n",
    "            if iteration % 5 == 0:\n",
    "                print(\"Completed iteration {}.  Loss: {}\".format(iteration, self.evaluate(self.data, self.Y)))\n",
    "                \n",
    "        return self.evaluate(self.data, self.Y)\n",
    "            \n",
    "    def predict(self, data=None):\n",
    "        self.assert_trained()\n",
    "        if data is None:\n",
    "            data = self.data\n",
    "        return self.iterations[-1].predict(data)\n",
    "    \n",
    "    def evaluate(self, data=None, Y=None):\n",
    "        self.assert_trained()\n",
    "        if data is None:\n",
    "            data = self.data\n",
    "            Y = self.Y\n",
    "        return self.iterations[-1].evaluate(data, Y)\n",
    "    \n",
    "    def assert_trained(self):\n",
    "        if self.iterations is None:\n",
    "            raise Exception(\"Must train before running `predict`.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Layer Version (Complete Version)\n",
    "\n",
    "Now we need to update Layer for backpropegation.  We can start by making an `update_weights` function that executes the core premise: \n",
    "\n",
    "    new_weights = old_weights - learning_rate * derivatives\n",
    "\n",
    "where *derivatives* is the derivative of the loss function with respect to the weights.  This is the step where neural network introductions appear to get complex because of the differential calculus.  However, we can always make a method to approximate the derivative.  Thus, we can (in theory) approximate a derivative for any loss function we seek to minimize.\n",
    "\n",
    "Better estimators are more involved so we'll simply look at the slope for the line between the points *W-e* and *W+e*, where *W* are the weights and *e* is some small number epsilon.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class Layer:\n",
    "    def __init__(self, model_iteration, size, activation_function, weights, betas):\n",
    "        self.model_iteration = model_iteration  # locator pattern\n",
    "        self.size = size\n",
    "        self.activation_function = activation_function\n",
    "        self.weights = weights\n",
    "        self.betas = betas\n",
    "        self.derivatives = None\n",
    "\n",
    "    def apply_weights(self, layer_input):\n",
    "        Z = np.dot(self.weights, layer_input) + self.betas\n",
    "        output = self.activation_function(Z)\n",
    "        return output\n",
    "    \n",
    "    def update_weights(self, learning_rate):\n",
    "        if self.derivatives is None:\n",
    "            self.calculate_derivatives()\n",
    "        self.weights = self.weights - learning_rate * self.derivatives\n",
    "\n",
    "    def update_derivatives(self, derivatives):\n",
    "        self.derivatives = derivatives\n",
    "        \n",
    "    def calculate_derivatives(self):\n",
    "        derivatives = []\n",
    "        offset = DERIVATIVE_OFFSET\n",
    "        \n",
    "        for i in range(self.weights.shape[0]):\n",
    "            for j in range(self.weights.shape[1]):\n",
    "                original_weight = self.weights[i][j]\n",
    "                \n",
    "                self.weights[i][j] = original_weight - offset\n",
    "                cost1 = self.model_iteration.evaluate()\n",
    "                \n",
    "                self.weights[i][j] = original_weight + offset\n",
    "                cost2 = self.model_iteration.evaluate()\n",
    "                \n",
    "                derivative = (cost2 - cost1) / (2 * DERIVATIVE_OFFSET)\n",
    "                self.weights[i][j] = original_weight\n",
    "                derivatives.append(derivative)\n",
    "        return np.array(derivatives).reshape(*self.weights.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activation Functions\n",
    "\n",
    "Our activation functions are generic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def relu_activation(x):\n",
    "    \"\"\"Vectorized relu activation function\n",
    "    :return: 0 if x is less than 0, x otherwise.\n",
    "    \"\"\"\n",
    "    return np.multiply(x, x >= 0)\n",
    "    \n",
    "def sigmoid_activation(x):\n",
    "    \"\"\"Vectorized sigmoid activation function\n",
    "    :return: sigmoid of x\n",
    "    \"\"\"\n",
    "    return 1. / (1 + np.exp(-x))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss Functions\n",
    "\n",
    "Because we used numerical methods to estimate the derivatives of the loss function, we aren't required to use a function whose derivative we know.  However, we'll use the same log loss regardless as it is appropriate to the domain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import log_loss\n",
    "\n",
    "def binary_loss_function(predictions, Y):\n",
    "    \"\"\"Loss function for a binary classifier\"\"\"\n",
    "    return log_loss(Y, (predictions[0] > 0.5).astype(int))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Putting It Together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn import preprocessing\n",
    "\n",
    "DERIVATIVE_OFFSET = 0.1\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "iris_x = preprocessing.scale(iris[\"data\"])\n",
    "iris_y = iris[\"target\"]\n",
    "Y2 = (iris_y == 0).astype(int)\n",
    "\n",
    "structure = [{\"size\": 3, \"activation\": relu_activation}, \n",
    "             {\"size\": 1, \"activation\": sigmoid_activation}]\n",
    "\n",
    "model = Model(iris_x, Y2, structure, binary_loss_function, learning_rate=0.001)\n",
    "model.train()\n",
    "\n",
    "\n",
    "\n",
    "  \n",
    "        \n",
    "    \n",
    "\n",
    "\n",
    "       \n",
    "        \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
