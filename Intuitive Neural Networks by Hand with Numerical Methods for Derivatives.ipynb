{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intuitive Neural Networks by Hand\n",
    "\n",
    "This notebook shows how to build a neural network by hand.  The motivation for creating _yet another_ neural network from scratch post is that I believe most get lost in the calculus and lose the concepts.  Additionally, I believe the demonstation code is often designed to be efficient with numpy, not illustrative.\n",
    "\n",
    "Below, we create classes to represent our neural network and then use it to fit to a simple example from the Iris dataset.\n",
    "\n",
    "* NOTE: Code is still under development and bugs likely exist."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Concepts\n",
    "\n",
    "Neural networks are essentially two things:\n",
    "1. A collection of node layers with connecting weights\n",
    "1. A forwards-backwards algorithm (of sorts) for optimizing those weights.\n",
    "\n",
    "The forward-backward algorithm is a common paradigm outside of neural networks and is helpful to learn about. By analogy, the forward step simply computes the current outcome probabilities.  The backwards step updates the weights used in making predictions to those that seem best at the current iteration.  This is the \"gradient descent\" that is frequently mentioned (and used).  The bidirectional process continues (hopefully) to convergence.\n",
    "\n",
    "At prediction time, one is essentially re-computing the forward step of the neural network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"static/neural-network.png\" alt=\"drawing\" width=\"500\"/>\n",
    "\n",
    "A key concept is that neural network weights are linear, with each node receiving a linear combination of the node outputs from the prior layer. For the first layer, nodes receive a linear combination of the dataset features.  For subsequent layers, nodes receive a linear combination of the activation functions of the previous nodes.  In this way, neural networks continually augment the data with linear combinations (followed by non-linear activation functions), projecting iterations of data into new dimensions until a good prediction is made.  As layers get arbitrarily large, one can imagine how these projections capture latent features.\n",
    "\n",
    "Now notice that the weights between these layers are simply edges on a bipartite graph.  This gives us our insight that we can represent the weights as a matrix between the current layer and the prior layer:\n",
    "\n",
    "<img src=\"static/bipartite-graph.png\" alt=\"drawing\" width=\"500\"/>\n",
    "\n",
    "Thus it follows that weighting the input is simply a matrix multiplication by the adjacency matrix: _W*input + b_."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Layer (Incomplete Version)\n",
    "\n",
    "The first thing that our neural network will need is a notion of a layer.  As with any coding project, we do not know our final state API *yet*, but we have a notion of how to start.  Namely, our Layer will need a size (number of nodes), an activation function, weights, and betas.  We also use a locator pattern to store to which model iteration this Layer belongs (note that we have not defined model iterations yet!).\n",
    "\n",
    "The IncompleteLayer class knows how to apply weights using our bipartite graph insight.  We simply multiply the weights by the input matrix and add our beta terms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class IncompleteLayer:\n",
    "    def __init__(self, model_iteration, size, activation_function, weights, betas):\n",
    "        self.model_iteration = model_iteration  # locator pattern\n",
    "        self.size = size\n",
    "        self.activation_function = activation_function\n",
    "        self.weights = weights\n",
    "        self.betas = betas\n",
    "\n",
    "    def apply_weights(self, layer_input):\n",
    "        \"\"\"Equivalent to a forward propegation on a single layer\"\"\"\n",
    "        Z = np.dot(self.weights, layer_input) + self.betas\n",
    "        output = self.activation_function(Z)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Iteration\n",
    "\n",
    "Every neural network model runs for multiple iterations and these iterations have their own layers, weights, etc.  Thus it makes for a good class to build.\n",
    "\n",
    "Our ModelIteration class should have a `feed_forward` function that runs our `Layer.apply_weights` and then returns the last value (the output of the last layer is the neural network output).  Similarly, predict is simply a call to `feed_forward` under this methodology.\n",
    "\n",
    "We will also need a way to do backprogegation.  The derivative calculations can be passed to Layers.  (Note that we have implicitly defined `Layer.update_weights`.)\n",
    "\n",
    "### Setting Weights\n",
    "\n",
    "Despite defining much of the model iterations and layers, we never wrote any code that sets the values of weights. \n",
    "\n",
    "**On any forward feeding of a model, the weights are defined to be either random initialization or the weights from the prior backpropegation.** Thus ModelIteration will need to loop to create the layers, each time looking at the prior iteration values:\n",
    "\n",
    "```\n",
    "for layer in model_structure:\n",
    "    if prior_iteration:\n",
    "        weights = prior_iteration.weights\n",
    "        betas = prior_iteration.betas\n",
    "    else:\n",
    "        weights = random_initialization()\n",
    "        betas = zeros()\n",
    "```\n",
    "\n",
    "A final wrinkle: To initialize the weights on the first iteration, we must be able to define the dimensions of the weight matrix.  From our bipartite graph, the matrix dimensions will be *(# current layer nodes, # prior layer nodes)*.  However, remember that the first layer has *# dataset features* columns as there is no prior layer.  The `__init__` method of the ModelIteration class updates the pseudocode to include these dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class ModelIteration:\n",
    "    def __init__(self, model, data, Y, learning_rate, prior_iteration=None):\n",
    "        self.model = model  # locator pattern\n",
    "        self.learning_rate = learning_rate\n",
    "        self.prior_iteration = prior_iteration\n",
    "        self.layers = []\n",
    "        \n",
    "        # iterate over each layer to initialize weights, per the above description.\n",
    "        for layer_number, layer in enumerate(self.model.model_structure):\n",
    "            if self.prior_iteration is None: # first iteration, must initialize weights\n",
    "                if 0 == layer_number:\n",
    "                    prior_layer_size = data.shape[1]\n",
    "                else:\n",
    "                    prior_layer_size = self.layers[-1].size\n",
    "                weights = np.random.randn(layer[\"size\"], prior_layer_size) / np.sqrt(layer[\"size\"])\n",
    "                betas = np.zeros((layer[\"size\"], 1))\n",
    "            else:\n",
    "                weights = self.prior_iteration.layers[layer_number].weights # backprop output\n",
    "                betas = self.prior_iteration.layers[layer_number].betas            \n",
    "           \n",
    "            layer = Layer(self, layer[\"size\"], layer[\"activation\"], weights, betas)\n",
    "            self.layers.append(layer)\n",
    "        \n",
    "    def feed_forward(self, data):\n",
    "        prior_output = self.model.data.T\n",
    "        for layer in self.layers:\n",
    "            output = layer.apply_weights(prior_output)\n",
    "            prior_output = output\n",
    "        return output\n",
    "    \n",
    "    def predict(self, data):\n",
    "         return self.feed_forward(data)\n",
    "        \n",
    "    def evaluate(self, data=None, Y=None):\n",
    "        if data is None:\n",
    "            data = self.model.data\n",
    "            Y = self.model.Y\n",
    "        predictions = self.predict(data)\n",
    "        return self.model.cost_function(predictions, Y)\n",
    "\n",
    "    def propegate_backward(self):\n",
    "        derivatives_list = []\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            derivatives = layer.calculate_derivatives()\n",
    "            derivatives_list.append(derivatives)\n",
    "            \n",
    "        for i, layer in enumerate(self.layers):\n",
    "            layer.update_derivatives(derivatives_list[i])\n",
    "            layer.update_weights(self.learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Class\n",
    "\n",
    "Our model class is effectively a wrapper for ModelIteration that calls the last iteration for `predict`.  The only difference is that our Model class needs a `train` method.  `train` will loop over model iterations, feed forward and back propegate, printing status along the way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class Model:\n",
    "    def __init__(self, data, Y, model_structure, cost_function, learning_rate):\n",
    "        self.data = data\n",
    "        self.Y = Y\n",
    "        self.model_structure = model_structure\n",
    "        self.cost_function = cost_function\n",
    "        self.learning_rate = learning_rate\n",
    "        self.iterations = None\n",
    "        \n",
    "    def train(self, learning_rate=0.01, num_iterations=5000):\n",
    "        self.iterations = []\n",
    "        prior_iteration = None\n",
    "        for iteration in range(num_iterations):\n",
    "            model_iteration = ModelIteration(self, self.data, self.Y, learning_rate, prior_iteration)\n",
    "            self.iterations.append(model_iteration)\n",
    "            \n",
    "            iteration_output = model_iteration.feed_forward(self.data)\n",
    "            model_iteration.propegate_backward() # update weights\n",
    "    \n",
    "            prior_iteration = model_iteration\n",
    "            \n",
    "            if iteration % 5 == 0:\n",
    "                print(\"Completed iteration {}.  Loss: {}\".format(iteration, self.evaluate(self.data, self.Y)))\n",
    "                \n",
    "        return self.evaluate(self.data, self.Y)\n",
    "            \n",
    "    def predict(self, data=None):\n",
    "        self.assert_trained()\n",
    "        if data is None:\n",
    "            data = self.data\n",
    "        return self.iterations[-1].predict(data)\n",
    "    \n",
    "    def evaluate(self, data=None, Y=None):\n",
    "        self.assert_trained()\n",
    "        if data is None:\n",
    "            data = self.data\n",
    "            Y = self.Y\n",
    "        return self.iterations[-1].evaluate(data, Y)\n",
    "    \n",
    "    def assert_trained(self):\n",
    "        if self.iterations is None:\n",
    "            raise Exception(\"Must train before running `predict`.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Layer Version (Complete Version)\n",
    "\n",
    "Now we need to update Layer for backpropegation.  We can start by making an `update_weights` function that executes the core premise: \n",
    "\n",
    "    new_weights = old_weights - learning_rate * derivatives\n",
    "\n",
    "where *derivatives* is the derivative of the loss function with respect to the weights.  This is the step where neural network introductions appear to get complex because of the differential calculus.  However, we can always make a method to approximate the derivative.  Thus, we can (in theory) approximate a derivative for any loss function we seek to minimize.\n",
    "\n",
    "Better estimators are more involved so we'll simply look at the slope for the line between the points *W-e* and *W+e*, where *W* are the weights and *e* is some small number epsilon.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class Layer:\n",
    "    def __init__(self, model_iteration, size, activation_function, weights, betas):\n",
    "        self.model_iteration = model_iteration  # locator pattern\n",
    "        self.size = size\n",
    "        self.activation_function = activation_function\n",
    "        self.weights = weights\n",
    "        self.betas = betas\n",
    "        self.derivatives = None\n",
    "\n",
    "    def apply_weights(self, layer_input):\n",
    "        Z = np.dot(self.weights, layer_input) + self.betas\n",
    "        output = self.activation_function(Z)\n",
    "        return output\n",
    "    \n",
    "    def update_weights(self, learning_rate):\n",
    "        if self.derivatives is None:\n",
    "            self.calculate_derivatives()\n",
    "        self.weights = self.weights - learning_rate * self.derivatives\n",
    "\n",
    "    def update_derivatives(self, derivatives):\n",
    "        self.derivatives = derivatives\n",
    "        \n",
    "    def calculate_derivatives(self):\n",
    "        derivatives = np.zeros(self.weights.shape)\n",
    "        offset = DERIVATIVE_OFFSET\n",
    "        \n",
    "        for i in range(self.weights.shape[0]):\n",
    "            for j in range(self.weights.shape[1]):\n",
    "                original_weight = self.weights[i][j]\n",
    "                \n",
    "                self.weights[i][j] = original_weight - offset\n",
    "                cost1 = self.model_iteration.evaluate()\n",
    "                \n",
    "                self.weights[i][j] = original_weight + offset\n",
    "                cost2 = self.model_iteration.evaluate()\n",
    "                \n",
    "                derivative = (cost2 - cost1) / (2 * DERIVATIVE_OFFSET)\n",
    "                self.weights[i][j] = original_weight\n",
    "                derivatives[i][j] = derivative\n",
    "        #return np.array(derivatives).reshape(*self.weights.shape)\n",
    "        return derivatives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activation Functions\n",
    "\n",
    "Our activation functions are generic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def relu_activation(x):\n",
    "    \"\"\"Vectorized relu activation function\n",
    "    :return: 0 if x is less than 0, x otherwise.\n",
    "    \"\"\"\n",
    "    return np.multiply(x, x >= 0)\n",
    "    \n",
    "def sigmoid_activation(x):\n",
    "    \"\"\"Vectorized sigmoid activation function\n",
    "    :return: sigmoid of x\n",
    "    \"\"\"\n",
    "    return 1. / (1 + np.exp(-x))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss Functions\n",
    "\n",
    "Because we used numerical methods to estimate the derivatives of the loss function, we aren't required to use a function whose derivative we know.  However, we'll use the same log loss regardless as it is appropriate to the domain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import log_loss\n",
    "\n",
    "def binary_loss_function(predictions, Y):\n",
    "    \"\"\"Loss function for a binary classifier\"\"\"\n",
    "    return log_loss(Y, (predictions[0] > 0.5).astype(int))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Putting It Together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed iteration 0.  Loss: 23.026383994893916\n",
      "Completed iteration 5.  Loss: 23.026383994893916\n",
      "Completed iteration 10.  Loss: 23.026383994893916\n",
      "Completed iteration 15.  Loss: 23.026383994893916\n",
      "Completed iteration 20.  Loss: 23.026383994893916\n",
      "Completed iteration 25.  Loss: 23.026383994893916\n",
      "Completed iteration 30.  Loss: 23.026383994893916\n",
      "Completed iteration 35.  Loss: 23.026383994893916\n",
      "Completed iteration 40.  Loss: 23.026383994893916\n",
      "Completed iteration 45.  Loss: 23.026383994893916\n",
      "Completed iteration 50.  Loss: 23.026383994893916\n",
      "Completed iteration 55.  Loss: 23.026383994893916\n",
      "Completed iteration 60.  Loss: 23.026383994893916\n",
      "Completed iteration 65.  Loss: 23.026383994893916\n",
      "Completed iteration 70.  Loss: 23.026383994893916\n",
      "Completed iteration 75.  Loss: 23.026383994893916\n",
      "Completed iteration 80.  Loss: 23.026383994893916\n",
      "Completed iteration 85.  Loss: 23.026383994893916\n",
      "Completed iteration 90.  Loss: 23.026383994893916\n",
      "Completed iteration 95.  Loss: 23.026383994893916\n",
      "Completed iteration 100.  Loss: 23.026383994893916\n",
      "Completed iteration 105.  Loss: 23.026383994893916\n",
      "Completed iteration 110.  Loss: 23.026383994893916\n",
      "Completed iteration 115.  Loss: 23.026383994893916\n",
      "Completed iteration 120.  Loss: 23.026383994893916\n",
      "Completed iteration 125.  Loss: 23.026383994893916\n",
      "Completed iteration 130.  Loss: 23.026383994893916\n",
      "Completed iteration 135.  Loss: 23.026383994893916\n",
      "Completed iteration 140.  Loss: 23.026383994893916\n",
      "Completed iteration 145.  Loss: 23.026383994893916\n",
      "Completed iteration 150.  Loss: 23.026383994893916\n",
      "Completed iteration 155.  Loss: 23.026383994893916\n",
      "Completed iteration 160.  Loss: 23.026383994893916\n",
      "Completed iteration 165.  Loss: 23.026383994893916\n",
      "Completed iteration 170.  Loss: 23.026383994893916\n",
      "Completed iteration 175.  Loss: 23.026383994893916\n",
      "Completed iteration 180.  Loss: 23.026383994893916\n",
      "Completed iteration 185.  Loss: 23.026383994893916\n",
      "Completed iteration 190.  Loss: 23.026383994893916\n",
      "Completed iteration 195.  Loss: 23.026383994893916\n",
      "Completed iteration 200.  Loss: 23.026383994893916\n",
      "Completed iteration 205.  Loss: 23.026383994893916\n",
      "Completed iteration 210.  Loss: 23.026383994893916\n",
      "Completed iteration 215.  Loss: 23.026383994893916\n",
      "Completed iteration 220.  Loss: 23.026383994893916\n",
      "Completed iteration 225.  Loss: 23.026383994893916\n",
      "Completed iteration 230.  Loss: 23.026383994893916\n",
      "Completed iteration 235.  Loss: 23.026383994893916\n",
      "Completed iteration 240.  Loss: 23.026383994893916\n",
      "Completed iteration 245.  Loss: 23.026383994893916\n",
      "Completed iteration 250.  Loss: 23.026383994893916\n",
      "Completed iteration 255.  Loss: 23.026383994893916\n",
      "Completed iteration 260.  Loss: 23.026383994893916\n",
      "Completed iteration 265.  Loss: 23.026383994893916\n",
      "Completed iteration 270.  Loss: 23.026383994893916\n",
      "Completed iteration 275.  Loss: 23.026383994893916\n",
      "Completed iteration 280.  Loss: 23.026383994893916\n",
      "Completed iteration 285.  Loss: 23.026383994893916\n",
      "Completed iteration 290.  Loss: 23.026383994893916\n",
      "Completed iteration 295.  Loss: 23.026383994893916\n",
      "Completed iteration 300.  Loss: 23.026383994893916\n",
      "Completed iteration 305.  Loss: 23.026383994893916\n",
      "Completed iteration 310.  Loss: 23.026383994893916\n",
      "Completed iteration 315.  Loss: 23.026383994893916\n",
      "Completed iteration 320.  Loss: 23.026383994893916\n",
      "Completed iteration 325.  Loss: 23.026383994893916\n",
      "Completed iteration 330.  Loss: 23.026383994893916\n",
      "Completed iteration 335.  Loss: 23.026383994893916\n",
      "Completed iteration 340.  Loss: 23.026383994893916\n",
      "Completed iteration 345.  Loss: 23.026383994893916\n",
      "Completed iteration 350.  Loss: 23.026383994893916\n",
      "Completed iteration 355.  Loss: 23.026383994893916\n",
      "Completed iteration 360.  Loss: 23.026383994893916\n",
      "Completed iteration 365.  Loss: 23.026383994893916\n",
      "Completed iteration 370.  Loss: 23.026383994893916\n",
      "Completed iteration 375.  Loss: 23.026383994893916\n",
      "Completed iteration 380.  Loss: 23.026383994893916\n",
      "Completed iteration 385.  Loss: 23.026383994893916\n",
      "Completed iteration 390.  Loss: 23.026383994893916\n",
      "Completed iteration 395.  Loss: 23.026383994893916\n",
      "Completed iteration 400.  Loss: 23.026383994893916\n",
      "Completed iteration 405.  Loss: 23.026383994893916\n",
      "Completed iteration 410.  Loss: 23.026383994893916\n",
      "Completed iteration 415.  Loss: 23.026383994893916\n",
      "Completed iteration 420.  Loss: 23.026383994893916\n",
      "Completed iteration 425.  Loss: 23.026383994893916\n",
      "Completed iteration 430.  Loss: 23.026383994893916\n",
      "Completed iteration 435.  Loss: 23.026383994893916\n",
      "Completed iteration 440.  Loss: 23.026383994893916\n",
      "Completed iteration 445.  Loss: 23.026383994893916\n",
      "Completed iteration 450.  Loss: 23.026383994893916\n",
      "Completed iteration 455.  Loss: 23.026383994893916\n",
      "Completed iteration 460.  Loss: 23.026383994893916\n",
      "Completed iteration 465.  Loss: 23.026383994893916\n",
      "Completed iteration 470.  Loss: 23.026383994893916\n",
      "Completed iteration 475.  Loss: 23.026383994893916\n",
      "Completed iteration 480.  Loss: 23.026383994893916\n",
      "Completed iteration 485.  Loss: 23.026383994893916\n",
      "Completed iteration 490.  Loss: 23.026383994893916\n",
      "Completed iteration 495.  Loss: 23.026383994893916\n",
      "Completed iteration 500.  Loss: 23.026383994893916\n",
      "Completed iteration 505.  Loss: 23.026383994893916\n",
      "Completed iteration 510.  Loss: 23.026383994893916\n",
      "Completed iteration 515.  Loss: 23.026383994893916\n",
      "Completed iteration 520.  Loss: 23.026383994893916\n",
      "Completed iteration 525.  Loss: 23.026383994893916\n",
      "Completed iteration 530.  Loss: 23.026383994893916\n",
      "Completed iteration 535.  Loss: 23.026383994893916\n",
      "Completed iteration 540.  Loss: 23.026383994893916\n",
      "Completed iteration 545.  Loss: 23.026383994893916\n",
      "Completed iteration 550.  Loss: 23.026383994893916\n",
      "Completed iteration 555.  Loss: 23.026383994893916\n",
      "Completed iteration 560.  Loss: 23.026383994893916\n",
      "Completed iteration 565.  Loss: 23.026383994893916\n",
      "Completed iteration 570.  Loss: 23.026383994893916\n",
      "Completed iteration 575.  Loss: 23.026383994893916\n",
      "Completed iteration 580.  Loss: 23.026383994893916\n",
      "Completed iteration 585.  Loss: 23.026383994893916\n",
      "Completed iteration 590.  Loss: 23.026383994893916\n",
      "Completed iteration 595.  Loss: 23.026383994893916\n",
      "Completed iteration 600.  Loss: 23.026383994893916\n",
      "Completed iteration 605.  Loss: 23.026383994893916\n",
      "Completed iteration 610.  Loss: 23.026383994893916\n",
      "Completed iteration 615.  Loss: 23.026383994893916\n",
      "Completed iteration 620.  Loss: 23.026383994893916\n",
      "Completed iteration 625.  Loss: 23.026383994893916\n",
      "Completed iteration 630.  Loss: 23.026383994893916\n",
      "Completed iteration 635.  Loss: 23.026383994893916\n",
      "Completed iteration 640.  Loss: 23.026383994893916\n",
      "Completed iteration 645.  Loss: 23.026383994893916\n",
      "Completed iteration 650.  Loss: 23.026383994893916\n",
      "Completed iteration 655.  Loss: 23.026383994893916\n",
      "Completed iteration 660.  Loss: 23.026383994893916\n",
      "Completed iteration 665.  Loss: 23.026383994893916\n",
      "Completed iteration 670.  Loss: 23.026383994893916\n",
      "Completed iteration 675.  Loss: 23.026383994893916\n",
      "Completed iteration 680.  Loss: 23.026383994893916\n",
      "Completed iteration 685.  Loss: 23.026383994893916\n",
      "Completed iteration 690.  Loss: 23.026383994893916\n",
      "Completed iteration 695.  Loss: 23.026383994893916\n",
      "Completed iteration 700.  Loss: 23.026383994893916\n",
      "Completed iteration 705.  Loss: 23.026383994893916\n",
      "Completed iteration 710.  Loss: 23.026383994893916\n",
      "Completed iteration 715.  Loss: 23.026383994893916\n",
      "Completed iteration 720.  Loss: 23.026383994893916\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-e499d7a599b0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miris_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstructure\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbinary_loss_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.001\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-1625b9e6dc49>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, learning_rate, num_iterations)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m             \u001b[0miteration_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_iteration\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeed_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m             \u001b[0mmodel_iteration\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpropegate_backward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# update weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m             \u001b[0mprior_iteration\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_iteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-20-fa35f01ae732>\u001b[0m in \u001b[0;36mpropegate_backward\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[0mderivatives_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m             \u001b[0mderivatives\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcalculate_derivatives\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m             \u001b[0mderivatives_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mderivatives\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-8-345c5e69bda4>\u001b[0m in \u001b[0;36mcalculate_derivatives\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moriginal_weight\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0moffset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m                 \u001b[0mcost1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_iteration\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moriginal_weight\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0moffset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-20-fa35f01ae732>\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self, data, Y)\u001b[0m\n\u001b[1;32m     39\u001b[0m             \u001b[0mY\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mY\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcost_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpropegate_backward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-cf2d5d11e5af>\u001b[0m in \u001b[0;36mbinary_loss_function\u001b[0;34m(predictions, Y)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mbinary_loss_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;34m\"\"\"Loss function for a binary classifier\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mlog_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/venv/default/lib/python3.6/site-packages/sklearn/metrics/classification.py\u001b[0m in \u001b[0;36mlog_loss\u001b[0;34m(y_true, y_pred, eps, normalize, sample_weight, labels)\u001b[0m\n\u001b[1;32m   2138\u001b[0m                              'got {0}.'.format(lb.classes_))\n\u001b[1;32m   2139\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2140\u001b[0;31m     \u001b[0mtransformed_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2141\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2142\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtransformed_labels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/venv/default/lib/python3.6/site-packages/sklearn/preprocessing/label.py\u001b[0m in \u001b[0;36mtransform\u001b[0;34m(self, y)\u001b[0m\n\u001b[1;32m    468\u001b[0m                               \u001b[0mpos_label\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpos_label\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    469\u001b[0m                               \u001b[0mneg_label\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mneg_label\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 470\u001b[0;31m                               sparse_output=self.sparse_output)\n\u001b[0m\u001b[1;32m    471\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    472\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0minverse_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthreshold\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/venv/default/lib/python3.6/site-packages/sklearn/preprocessing/label.py\u001b[0m in \u001b[0;36mlabel_binarize\u001b[0;34m(y, classes, neg_label, pos_label, sparse_output)\u001b[0m\n\u001b[1;32m    643\u001b[0m         \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfill\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpos_label\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    644\u001b[0m         Y = sp.csr_matrix((data, indices, indptr),\n\u001b[0;32m--> 645\u001b[0;31m                           shape=(n_samples, n_classes))\n\u001b[0m\u001b[1;32m    646\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0my_type\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"multilabel-indicator\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    647\u001b[0m         \u001b[0mY\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcsr_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/venv/default/lib/python3.6/site-packages/scipy/sparse/compressed.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, arg1, shape, dtype, copy)\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0misshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m                 \u001b[0;31m# It's a tuple of matrix dimensions (M, N)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m                 \u001b[0;31m# create empty matrix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn import preprocessing\n",
    "\n",
    "DERIVATIVE_OFFSET = 0.05\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "iris_x = preprocessing.scale(iris[\"data\"])\n",
    "iris_y = iris[\"target\"]\n",
    "Y2 = (iris_y == 0).astype(int)\n",
    "\n",
    "structure = [{\"size\": 3, \"activation\": sigmoid_activation}, \n",
    "             {\"size\": 1, \"activation\": sigmoid_activation}]\n",
    "\n",
    "model = Model(iris_x, Y2, structure, binary_loss_function, learning_rate=0.001)\n",
    "model.train()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "       \n",
    "        \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
