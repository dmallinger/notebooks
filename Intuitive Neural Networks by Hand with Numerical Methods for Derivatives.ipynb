{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intuitive Neural Networks by Hand\n",
    "\n",
    "This notebook shows how to build a neural network by hand.  The motivation for creating _yet another_ neural network from scratch post is that I believe most get lost in the calculus and lose the concepts.  Additionally, I believe the demonstation code is often designed to be efficient with numpy, not illustrative.\n",
    "\n",
    "Below, we create classes to represent our neural network and then use it to fit to a simple example from the Iris dataset.\n",
    "\n",
    "* NOTE: Code is still under development and bugs likely exist."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Concepts\n",
    "\n",
    "Neural networks are essentially two things:\n",
    "1. A collection of node layers with connecting weights\n",
    "1. A forwards-backwards algorithm (of sorts) for optimizing those weights.\n",
    "\n",
    "The forward-backward algorithm is a common paradigm outside of neural networks and is helpful to learn about. By analogy, the forward step simply computes the current outcome probabilities.  The backwards step updates the weights used in making predictions to those that seem best at the current iteration.  This is the \"gradient descent\" that is frequently mentioned (and used).  The bidirectional process continues (hopefully) to convergence.\n",
    "\n",
    "At prediction time, one is essentially re-computing the forward step of the neural network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"static/neural-network.png\" alt=\"drawing\" width=\"500\"/>\n",
    "\n",
    "A key concept is that neural network weights are linear, with each node receiving a linear combination of the node outputs from the prior layer. For the first layer, nodes receive a linear combination of the dataset features.  For subsequent layers, nodes receive a linear combination of the activation functions of the previous nodes.  In this way, neural networks continually augment the data with linear combinations (followed by non-linear activation functions), projecting iterations of data into new dimensions until a good prediction is made.  As layers get arbitrarily large, one can imagine how these projections capture latent features.\n",
    "\n",
    "Now notice that the weights between these layers are simply edges on a bipartite graph.  This gives us our insight that we can represent the weights as a matrix between the current layer and the prior layer:\n",
    "\n",
    "<img src=\"static/bipartite-graph.png\" alt=\"drawing\" width=\"500\"/>\n",
    "\n",
    "Thus it follows that weighting the input is simply a matrix multiplication by the adjacency matrix: _W*input + b_."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Layer (Incomplete Version)\n",
    "\n",
    "The first thing that our neural network will need is a notion of a layer.  As with any coding project, we do not know our final state API *yet*, but we have a notion of how to start.  Namely, our Layer will need a size (number of nodes), an activation function, weights, and betas.  We also use a locator pattern to store to which model iteration this Layer belongs (note that we have not defined model iterations yet!).\n",
    "\n",
    "The IncompleteLayer class knows how to apply weights using our bipartite graph insight.  We simply multiply the weights by the input matrix and add our beta terms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class IncompleteLayer:\n",
    "    def __init__(self, model_iteration, size, activation_function, weights, betas):\n",
    "        self.model_iteration = model_iteration  # locator pattern\n",
    "        self.size = size\n",
    "        self.activation_function = activation_function\n",
    "        self.weights = weights\n",
    "        self.betas = betas\n",
    "\n",
    "    def apply_weights(self, layer_input):\n",
    "        \"\"\"Equivalent to a forward propegation on a single layer\"\"\"\n",
    "        Z = np.dot(self.weights, layer_input) + self.betas\n",
    "        output = self.activation_function(Z)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Iteration\n",
    "\n",
    "Every neural network model runs for multiple iterations and these iterations have their own layers, weights, etc.  Thus it makes for a good class to build.\n",
    "\n",
    "Our ModelIteration class should have a `feed_forward` function that runs our `Layer.apply_weights` and then returns the last value (the output of the last layer is the neural network output).  Similarly, predict is simply a call to `feed_forward` under this methodology.\n",
    "\n",
    "We will also need a way to do backprogegation.  The derivative calculations can be passed to Layers.  (Note that we have implicitly defined `Layer.update_weights`.)\n",
    "\n",
    "### Setting Weights\n",
    "\n",
    "Despite defining much of the model iterations and layers, we never wrote any code that sets the values of weights. \n",
    "\n",
    "**On any forward feeding of a model, the weights are defined to be either random initialization or the weights from the prior backpropegation.** Thus ModelIteration will need to loop to create the layers, each time looking at the prior iteration values:\n",
    "\n",
    "```\n",
    "for layer in model_structure:\n",
    "    if prior_iteration:\n",
    "        weights = prior_iteration.weights\n",
    "        betas = prior_iteration.betas\n",
    "    else:\n",
    "        weights = random_initialization()\n",
    "        betas = zeros()\n",
    "```\n",
    "\n",
    "A final wrinkle: To initialize the weights on the first iteration, we must be able to define the dimensions of the weight matrix.  From our bipartite graph, the matrix dimensions will be *(# current layer nodes, # prior layer nodes)*.  However, remember that the first layer has *# dataset features* columns as there is no prior layer.  The `__init__` method of the ModelIteration class updates the pseudocode to include these dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class ModelIteration:\n",
    "    def __init__(self, model, data, Y, learning_rate, prior_iteration=None):\n",
    "        self.model = model  # locator pattern\n",
    "        # don't store data again, just wasteful\n",
    "        self.learning_rate = learning_rate\n",
    "        self.prior_iteration = prior_iteration\n",
    "        self.layers = []\n",
    "        for layer_number, layer in enumerate(self.model.model_structure):\n",
    "            if self.prior_iteration is None: # first iteration, must initialize weights\n",
    "                if 0 == layer_number:\n",
    "                    prior_layer_size = data.shape[1]\n",
    "                else:\n",
    "                    prior_layer_size = self.layers[-1].size\n",
    "                weights = np.random.randn(layer[\"size\"], prior_layer_size)\n",
    "                betas = np.zeros((layer[\"size\"], 1))\n",
    "            else:\n",
    "                weights = self.prior_iteration.layers[layer_number].weights # backprop output\n",
    "                betas = self.prior_iteration.layers[layer_number].betas            \n",
    "           \n",
    "            layer = Layer(self, layer[\"size\"], layer[\"activation\"], weights, betas)\n",
    "            self.layers.append(layer)\n",
    "        \n",
    "    def feed_forward(self, data):\n",
    "        prior_output = self.model.data.T\n",
    "        for layer in self.layers:\n",
    "            output = layer.apply_weights(prior_output)\n",
    "            prior_output = output\n",
    "        return output\n",
    "    \n",
    "    def predict(self, data):\n",
    "         return self.feed_forward(data)\n",
    "        \n",
    "    def evaluate(self, data=None, Y=None):\n",
    "        if data is None:\n",
    "            data = self.model.data\n",
    "            Y = self.model.Y\n",
    "        predictions = self.predict(data)\n",
    "        return self.model.cost_function(predictions, Y)\n",
    "\n",
    "    def propegate_backward(self):\n",
    "        # iterate backwards\n",
    "        derivatives_list = []\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            derivatives = layer.calculate_derivatives()\n",
    "            derivatives_list.append(derivatives)\n",
    "            \n",
    "        for i, layer in enumerate(self.layers):\n",
    "            layer.update_derivatives(derivatives_list[i])\n",
    "            layer.update_weights(self.learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Class\n",
    "\n",
    "Our model class is effectively a wrapper for ModelIteration that calls the last iteration for `predict`.  The only difference is that our Model class needs a `train` method.  `train` will loop over model iterations, feed forward and back propegate, printing status along the way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class Model:\n",
    "    def __init__(self, data, Y, model_structure, cost_function, learning_rate):\n",
    "        self.data = data\n",
    "        self.Y = Y\n",
    "        self.model_structure = model_structure\n",
    "        self.cost_function = cost_function\n",
    "        self.learning_rate = learning_rate\n",
    "        self.iterations = None\n",
    "        \n",
    "    def train(self, learning_rate=0.01, num_iterations=5000):\n",
    "        self.iterations = []\n",
    "        prior_iteration = None\n",
    "        for iteration in range(num_iterations):\n",
    "            model_iteration = ModelIteration(self, self.data, self.Y, learning_rate, prior_iteration)\n",
    "            self.iterations.append(model_iteration)\n",
    "            \n",
    "            iteration_output = model_iteration.feed_forward(self.data)\n",
    "            model_iteration.propegate_backward() # update weights\n",
    "    \n",
    "            prior_iteration = model_iteration\n",
    "            \n",
    "            if iteration % 5 == 0:\n",
    "                print(\"Completed iteration {}.  Loss: {}\".format(iteration, self.evaluate(self.data, self.Y)))\n",
    "                \n",
    "        return self.evaluate(self.data, self.Y)\n",
    "            \n",
    "    def predict(self, data=None):\n",
    "        self.assert_trained()\n",
    "        if data is None:\n",
    "            data = self.data\n",
    "        return self.iterations[-1].predict(data)\n",
    "    \n",
    "    def evaluate(self, data=None, Y=None):\n",
    "        self.assert_trained()\n",
    "        if data is None:\n",
    "            data = self.data\n",
    "            Y = self.Y\n",
    "        return self.iterations[-1].evaluate(data, Y)\n",
    "    \n",
    "    def assert_trained(self):\n",
    "        if self.iterations is None:\n",
    "            raise Exception(\"Must train before running `predict`.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Layer Version (Complete Version)\n",
    "\n",
    "Now we need to update Layer for backpropegation.  We can start by making an `update_weights` function that executes the core premise: \n",
    "\n",
    "    new_weights = old_weights - learning_rate * derivatives\n",
    "\n",
    "where *derivatives* is the derivative of the loss function with respect to the weights.  This is the step where neural network introductions appear to get complex because of the differential calculus.  However, we can always make a method to approximate the derivative.  Thus, we can (in theory) approximate a derivative for any loss function we seek to minimize.\n",
    "\n",
    "Better estimators are more involved so we'll simply look at the slope for the line between the points *W-e* and *W+e*, where *W* are the weights and *e* is some small number epsilon.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class Layer:\n",
    "    def __init__(self, model_iteration, size, activation_function, weights, betas):\n",
    "        self.model_iteration = model_iteration  # locator pattern\n",
    "        self.size = size\n",
    "        self.activation_function = activation_function\n",
    "        self.weights = weights\n",
    "        self.betas = betas\n",
    "        self.derivatives = None\n",
    "\n",
    "    def apply_weights(self, layer_input):\n",
    "        Z = np.dot(self.weights, layer_input) + self.betas\n",
    "        output = self.activation_function(Z)\n",
    "        return output\n",
    "    \n",
    "    def update_weights(self, learning_rate):\n",
    "        if self.derivatives is None:\n",
    "            self.calculate_derivatives()\n",
    "        self.weights = self.weights - learning_rate * self.derivatives\n",
    "\n",
    "    def update_derivatives(self, derivatives):\n",
    "        self.derivatives = derivatives\n",
    "        \n",
    "    def calculate_derivatives(self):\n",
    "        # add epsilon and substract epsilon from weights and re-run....\n",
    "        derivatives = []\n",
    "        offset = DERIVATIVE_OFFSET\n",
    "        \n",
    "        for i in range(self.weights.shape[0]):\n",
    "            for j in range(self.weights.shape[1]):\n",
    "                original_weight = self.weights[i][j]\n",
    "                \n",
    "                self.weights[i][j] = original_weight - offset\n",
    "                cost1 = self.model_iteration.evaluate()\n",
    "                \n",
    "                self.weights[i][j] = original_weight + offset\n",
    "                cost2 = self.model_iteration.evaluate()\n",
    "                \n",
    "                derivative = (cost2 - cost1) / (2 * DERIVATIVE_OFFSET)\n",
    "                self.weights[i][j] = original_weight\n",
    "                derivatives.append(derivative)\n",
    "        return np.array(derivatives).reshape(*self.weights.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activation Functions\n",
    "\n",
    "Our activation functions are generic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def relu_activation(x):\n",
    "    \"\"\"Vectorized relu activation function\n",
    "    :return: 0 if x is less than 0, x otherwise.\n",
    "    \"\"\"\n",
    "    return np.multiply(x, x >= 0)\n",
    "    \n",
    "def sigmoid_activation(x):\n",
    "    \"\"\"Vectorized sigmoid activation function\n",
    "    :return: sigmoid of x\n",
    "    \"\"\"\n",
    "    return 1. / (1 + np.exp(-x))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss Functions\n",
    "\n",
    "Because we used numerical methods to estimate the derivatives of the loss function, we aren't required to use a function whose derivative we know.  However, we'll use the same log loss regardless as it is appropriate to the domain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import log_loss\n",
    "\n",
    "def binary_loss_function(predictions, Y):\n",
    "    \"\"\"Loss function for a binary classifier\"\"\"\n",
    "    return log_loss(Y, (predictions[0] > 0.5).astype(int))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Putting It Together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed iteration 0.  Loss: 6.677651358519237\n",
      "Completed iteration 5.  Loss: 6.677651358519237\n",
      "Completed iteration 10.  Loss: 6.677651358519237\n",
      "Completed iteration 15.  Loss: 6.677651358519237\n",
      "Completed iteration 20.  Loss: 6.677651358519237\n",
      "Completed iteration 25.  Loss: 6.677651358519237\n",
      "Completed iteration 30.  Loss: 6.677651358519237\n",
      "Completed iteration 35.  Loss: 6.677651358519237\n",
      "Completed iteration 40.  Loss: 6.677651358519237\n",
      "Completed iteration 45.  Loss: 6.677651358519237\n",
      "Completed iteration 50.  Loss: 6.677651358519237\n",
      "Completed iteration 55.  Loss: 6.677651358519237\n",
      "Completed iteration 60.  Loss: 6.677651358519237\n",
      "Completed iteration 65.  Loss: 6.677651358519237\n",
      "Completed iteration 70.  Loss: 6.677651358519237\n",
      "Completed iteration 75.  Loss: 6.677651358519237\n",
      "Completed iteration 80.  Loss: 6.677651358519237\n",
      "Completed iteration 85.  Loss: 6.677651358519237\n",
      "Completed iteration 90.  Loss: 6.677651358519237\n",
      "Completed iteration 95.  Loss: 6.677651358519237\n",
      "Completed iteration 100.  Loss: 6.677651358519237\n",
      "Completed iteration 105.  Loss: 6.677651358519237\n",
      "Completed iteration 110.  Loss: 6.677651358519237\n",
      "Completed iteration 115.  Loss: 6.677651358519237\n",
      "Completed iteration 120.  Loss: 6.677651358519237\n",
      "Completed iteration 125.  Loss: 6.677651358519237\n",
      "Completed iteration 130.  Loss: 6.677651358519237\n",
      "Completed iteration 135.  Loss: 6.677651358519237\n",
      "Completed iteration 140.  Loss: 6.677651358519237\n",
      "Completed iteration 145.  Loss: 6.677651358519237\n",
      "Completed iteration 150.  Loss: 6.677651358519237\n",
      "Completed iteration 155.  Loss: 6.677651358519237\n",
      "Completed iteration 160.  Loss: 6.677651358519237\n",
      "Completed iteration 165.  Loss: 6.677651358519237\n",
      "Completed iteration 170.  Loss: 6.677651358519237\n",
      "Completed iteration 175.  Loss: 6.677651358519237\n",
      "Completed iteration 180.  Loss: 6.677651358519237\n",
      "Completed iteration 185.  Loss: 6.677651358519237\n",
      "Completed iteration 190.  Loss: 6.677651358519237\n",
      "Completed iteration 195.  Loss: 6.677651358519237\n",
      "Completed iteration 200.  Loss: 6.677651358519237\n",
      "Completed iteration 205.  Loss: 6.677651358519237\n",
      "Completed iteration 210.  Loss: 6.677651358519237\n",
      "Completed iteration 215.  Loss: 6.677651358519237\n",
      "Completed iteration 220.  Loss: 6.677651358519237\n",
      "Completed iteration 225.  Loss: 6.677651358519237\n",
      "Completed iteration 230.  Loss: 6.677651358519237\n",
      "Completed iteration 235.  Loss: 6.677651358519237\n",
      "Completed iteration 240.  Loss: 6.677651358519237\n",
      "Completed iteration 245.  Loss: 6.677651358519237\n",
      "Completed iteration 250.  Loss: 6.677651358519237\n",
      "Completed iteration 255.  Loss: 6.677651358519237\n",
      "Completed iteration 260.  Loss: 6.677651358519237\n",
      "Completed iteration 265.  Loss: 6.677651358519237\n",
      "Completed iteration 270.  Loss: 6.677651358519237\n",
      "Completed iteration 275.  Loss: 6.677651358519237\n",
      "Completed iteration 280.  Loss: 6.677651358519237\n",
      "Completed iteration 285.  Loss: 6.677651358519237\n",
      "Completed iteration 290.  Loss: 6.677651358519237\n",
      "Completed iteration 295.  Loss: 6.677651358519237\n",
      "Completed iteration 300.  Loss: 6.677651358519237\n",
      "Completed iteration 305.  Loss: 6.677651358519237\n",
      "Completed iteration 310.  Loss: 6.677651358519237\n",
      "Completed iteration 315.  Loss: 6.677651358519237\n",
      "Completed iteration 320.  Loss: 6.677651358519237\n",
      "Completed iteration 325.  Loss: 6.677651358519237\n",
      "Completed iteration 330.  Loss: 6.677651358519237\n",
      "Completed iteration 335.  Loss: 6.677651358519237\n",
      "Completed iteration 340.  Loss: 6.677651358519237\n",
      "Completed iteration 345.  Loss: 6.677651358519237\n",
      "Completed iteration 350.  Loss: 6.677651358519237\n",
      "Completed iteration 355.  Loss: 6.677651358519237\n",
      "Completed iteration 360.  Loss: 6.677651358519237\n",
      "Completed iteration 365.  Loss: 6.677651358519237\n",
      "Completed iteration 370.  Loss: 6.677651358519237\n",
      "Completed iteration 375.  Loss: 6.677651358519237\n",
      "Completed iteration 380.  Loss: 6.677651358519237\n",
      "Completed iteration 385.  Loss: 6.677651358519237\n",
      "Completed iteration 390.  Loss: 6.677651358519237\n",
      "Completed iteration 395.  Loss: 6.677651358519237\n",
      "Completed iteration 400.  Loss: 6.677651358519237\n",
      "Completed iteration 405.  Loss: 6.677651358519237\n",
      "Completed iteration 410.  Loss: 6.677651358519237\n",
      "Completed iteration 415.  Loss: 6.677651358519237\n",
      "Completed iteration 420.  Loss: 6.677651358519237\n",
      "Completed iteration 425.  Loss: 6.677651358519237\n",
      "Completed iteration 430.  Loss: 6.677651358519237\n",
      "Completed iteration 435.  Loss: 6.677651358519237\n",
      "Completed iteration 440.  Loss: 6.677651358519237\n",
      "Completed iteration 445.  Loss: 6.677651358519237\n",
      "Completed iteration 450.  Loss: 6.677651358519237\n",
      "Completed iteration 455.  Loss: 6.677651358519237\n",
      "Completed iteration 460.  Loss: 6.677651358519237\n",
      "Completed iteration 465.  Loss: 6.677651358519237\n",
      "Completed iteration 470.  Loss: 6.677651358519237\n",
      "Completed iteration 475.  Loss: 6.677651358519237\n",
      "Completed iteration 480.  Loss: 6.677651358519237\n",
      "Completed iteration 485.  Loss: 6.677651358519237\n",
      "Completed iteration 490.  Loss: 6.677651358519237\n",
      "Completed iteration 495.  Loss: 6.677651358519237\n",
      "Completed iteration 500.  Loss: 6.677651358519237\n",
      "Completed iteration 505.  Loss: 6.677651358519237\n",
      "Completed iteration 510.  Loss: 6.677651358519237\n",
      "Completed iteration 515.  Loss: 6.677651358519237\n",
      "Completed iteration 520.  Loss: 6.677651358519237\n",
      "Completed iteration 525.  Loss: 6.677651358519237\n",
      "Completed iteration 530.  Loss: 6.677651358519237\n",
      "Completed iteration 535.  Loss: 6.677651358519237\n",
      "Completed iteration 540.  Loss: 6.677651358519237\n",
      "Completed iteration 545.  Loss: 6.677651358519237\n",
      "Completed iteration 550.  Loss: 6.677651358519237\n",
      "Completed iteration 555.  Loss: 6.677651358519237\n",
      "Completed iteration 560.  Loss: 6.677651358519237\n",
      "Completed iteration 565.  Loss: 6.677651358519237\n",
      "Completed iteration 570.  Loss: 6.677651358519237\n",
      "Completed iteration 575.  Loss: 6.677651358519237\n",
      "Completed iteration 580.  Loss: 6.677651358519237\n",
      "Completed iteration 585.  Loss: 6.677651358519237\n",
      "Completed iteration 590.  Loss: 6.677651358519237\n",
      "Completed iteration 595.  Loss: 6.677651358519237\n",
      "Completed iteration 600.  Loss: 6.677651358519237\n",
      "Completed iteration 605.  Loss: 6.677651358519237\n",
      "Completed iteration 610.  Loss: 6.677651358519237\n",
      "Completed iteration 615.  Loss: 6.677651358519237\n",
      "Completed iteration 620.  Loss: 6.677651358519237\n",
      "Completed iteration 625.  Loss: 6.677651358519237\n",
      "Completed iteration 630.  Loss: 6.677651358519237\n",
      "Completed iteration 635.  Loss: 6.677651358519237\n",
      "Completed iteration 640.  Loss: 6.677651358519237\n",
      "Completed iteration 645.  Loss: 6.677651358519237\n",
      "Completed iteration 650.  Loss: 6.677651358519237\n",
      "Completed iteration 655.  Loss: 6.677651358519237\n",
      "Completed iteration 660.  Loss: 6.677651358519237\n",
      "Completed iteration 665.  Loss: 6.677651358519237\n",
      "Completed iteration 670.  Loss: 6.677651358519237\n",
      "Completed iteration 675.  Loss: 6.677651358519237\n",
      "Completed iteration 680.  Loss: 6.677651358519237\n",
      "Completed iteration 685.  Loss: 6.677651358519237\n",
      "Completed iteration 690.  Loss: 6.677651358519237\n",
      "Completed iteration 695.  Loss: 6.677651358519237\n",
      "Completed iteration 700.  Loss: 6.677651358519237\n",
      "Completed iteration 705.  Loss: 6.677651358519237\n",
      "Completed iteration 710.  Loss: 6.677651358519237\n",
      "Completed iteration 715.  Loss: 6.677651358519237\n",
      "Completed iteration 720.  Loss: 6.677651358519237\n",
      "Completed iteration 725.  Loss: 6.677651358519237\n",
      "Completed iteration 730.  Loss: 6.677651358519237\n",
      "Completed iteration 735.  Loss: 6.677651358519237\n",
      "Completed iteration 740.  Loss: 6.677651358519237\n",
      "Completed iteration 745.  Loss: 6.677651358519237\n",
      "Completed iteration 750.  Loss: 6.677651358519237\n",
      "Completed iteration 755.  Loss: 6.677651358519237\n",
      "Completed iteration 760.  Loss: 6.677651358519237\n",
      "Completed iteration 765.  Loss: 6.677651358519237\n",
      "Completed iteration 770.  Loss: 6.677651358519237\n",
      "Completed iteration 775.  Loss: 6.677651358519237\n",
      "Completed iteration 780.  Loss: 6.677651358519237\n",
      "Completed iteration 785.  Loss: 6.677651358519237\n",
      "Completed iteration 790.  Loss: 6.677651358519237\n",
      "Completed iteration 795.  Loss: 6.677651358519237\n",
      "Completed iteration 800.  Loss: 6.677651358519237\n",
      "Completed iteration 805.  Loss: 6.677651358519237\n",
      "Completed iteration 810.  Loss: 6.677651358519237\n",
      "Completed iteration 815.  Loss: 6.677651358519237\n",
      "Completed iteration 820.  Loss: 6.677651358519237\n",
      "Completed iteration 825.  Loss: 6.677651358519237\n",
      "Completed iteration 830.  Loss: 6.677651358519237\n",
      "Completed iteration 835.  Loss: 6.677651358519237\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed iteration 840.  Loss: 6.677651358519237\n",
      "Completed iteration 845.  Loss: 6.677651358519237\n",
      "Completed iteration 850.  Loss: 6.677651358519237\n",
      "Completed iteration 855.  Loss: 6.677651358519237\n",
      "Completed iteration 860.  Loss: 6.677651358519237\n",
      "Completed iteration 865.  Loss: 6.677651358519237\n",
      "Completed iteration 870.  Loss: 6.677651358519237\n",
      "Completed iteration 875.  Loss: 6.677651358519237\n",
      "Completed iteration 880.  Loss: 6.677651358519237\n",
      "Completed iteration 885.  Loss: 6.677651358519237\n",
      "Completed iteration 890.  Loss: 6.677651358519237\n",
      "Completed iteration 895.  Loss: 6.677651358519237\n",
      "Completed iteration 900.  Loss: 6.677651358519237\n",
      "Completed iteration 905.  Loss: 6.677651358519237\n",
      "Completed iteration 910.  Loss: 6.677651358519237\n",
      "Completed iteration 915.  Loss: 6.677651358519237\n",
      "Completed iteration 920.  Loss: 6.677651358519237\n",
      "Completed iteration 925.  Loss: 6.677651358519237\n",
      "Completed iteration 930.  Loss: 6.677651358519237\n",
      "Completed iteration 935.  Loss: 6.677651358519237\n",
      "Completed iteration 940.  Loss: 6.677651358519237\n",
      "Completed iteration 945.  Loss: 6.677651358519237\n",
      "Completed iteration 950.  Loss: 6.677651358519237\n",
      "Completed iteration 955.  Loss: 6.677651358519237\n",
      "Completed iteration 960.  Loss: 6.677651358519237\n",
      "Completed iteration 965.  Loss: 6.677651358519237\n",
      "Completed iteration 970.  Loss: 6.677651358519237\n",
      "Completed iteration 975.  Loss: 6.677651358519237\n",
      "Completed iteration 980.  Loss: 6.677651358519237\n",
      "Completed iteration 985.  Loss: 6.677651358519237\n",
      "Completed iteration 990.  Loss: 6.677651358519237\n",
      "Completed iteration 995.  Loss: 6.677651358519237\n",
      "Completed iteration 1000.  Loss: 6.677651358519237\n",
      "Completed iteration 1005.  Loss: 6.677651358519237\n",
      "Completed iteration 1010.  Loss: 6.677651358519237\n",
      "Completed iteration 1015.  Loss: 6.677651358519237\n",
      "Completed iteration 1020.  Loss: 6.677651358519237\n",
      "Completed iteration 1025.  Loss: 6.677651358519237\n",
      "Completed iteration 1030.  Loss: 6.677651358519237\n",
      "Completed iteration 1035.  Loss: 6.677651358519237\n",
      "Completed iteration 1040.  Loss: 6.677651358519237\n",
      "Completed iteration 1045.  Loss: 6.677651358519237\n",
      "Completed iteration 1050.  Loss: 6.677651358519237\n",
      "Completed iteration 1055.  Loss: 6.677651358519237\n",
      "Completed iteration 1060.  Loss: 6.677651358519237\n",
      "Completed iteration 1065.  Loss: 6.677651358519237\n",
      "Completed iteration 1070.  Loss: 6.677651358519237\n",
      "Completed iteration 1075.  Loss: 6.677651358519237\n",
      "Completed iteration 1080.  Loss: 6.677651358519237\n",
      "Completed iteration 1085.  Loss: 6.677651358519237\n",
      "Completed iteration 1090.  Loss: 6.677651358519237\n",
      "Completed iteration 1095.  Loss: 6.677651358519237\n",
      "Completed iteration 1100.  Loss: 6.677651358519237\n",
      "Completed iteration 1105.  Loss: 6.677651358519237\n",
      "Completed iteration 1110.  Loss: 6.677651358519237\n",
      "Completed iteration 1115.  Loss: 6.677651358519237\n",
      "Completed iteration 1120.  Loss: 6.677651358519237\n",
      "Completed iteration 1125.  Loss: 6.677651358519237\n",
      "Completed iteration 1130.  Loss: 6.677651358519237\n",
      "Completed iteration 1135.  Loss: 6.677651358519237\n",
      "Completed iteration 1140.  Loss: 6.677651358519237\n",
      "Completed iteration 1145.  Loss: 6.677651358519237\n",
      "Completed iteration 1150.  Loss: 6.677651358519237\n",
      "Completed iteration 1155.  Loss: 6.677651358519237\n",
      "Completed iteration 1160.  Loss: 6.677651358519237\n",
      "Completed iteration 1165.  Loss: 6.677651358519237\n",
      "Completed iteration 1170.  Loss: 6.677651358519237\n",
      "Completed iteration 1175.  Loss: 6.677651358519237\n",
      "Completed iteration 1180.  Loss: 6.677651358519237\n",
      "Completed iteration 1185.  Loss: 6.677651358519237\n",
      "Completed iteration 1190.  Loss: 6.677651358519237\n",
      "Completed iteration 1195.  Loss: 6.677651358519237\n",
      "Completed iteration 1200.  Loss: 6.677651358519237\n",
      "Completed iteration 1205.  Loss: 6.677651358519237\n",
      "Completed iteration 1210.  Loss: 6.677651358519237\n",
      "Completed iteration 1215.  Loss: 6.677651358519237\n",
      "Completed iteration 1220.  Loss: 6.677651358519237\n",
      "Completed iteration 1225.  Loss: 6.677651358519237\n",
      "Completed iteration 1230.  Loss: 6.677651358519237\n",
      "Completed iteration 1235.  Loss: 6.677651358519237\n",
      "Completed iteration 1240.  Loss: 6.677651358519237\n",
      "Completed iteration 1245.  Loss: 6.677651358519237\n",
      "Completed iteration 1250.  Loss: 6.677651358519237\n",
      "Completed iteration 1255.  Loss: 6.677651358519237\n",
      "Completed iteration 1260.  Loss: 6.677651358519237\n",
      "Completed iteration 1265.  Loss: 6.677651358519237\n",
      "Completed iteration 1270.  Loss: 6.677651358519237\n",
      "Completed iteration 1275.  Loss: 6.677651358519237\n",
      "Completed iteration 1280.  Loss: 6.677651358519237\n",
      "Completed iteration 1285.  Loss: 6.677651358519237\n",
      "Completed iteration 1290.  Loss: 6.677651358519237\n",
      "Completed iteration 1295.  Loss: 6.677651358519237\n",
      "Completed iteration 1300.  Loss: 6.677651358519237\n",
      "Completed iteration 1305.  Loss: 6.677651358519237\n",
      "Completed iteration 1310.  Loss: 6.677651358519237\n",
      "Completed iteration 1315.  Loss: 6.677651358519237\n",
      "Completed iteration 1320.  Loss: 6.677651358519237\n",
      "Completed iteration 1325.  Loss: 6.677651358519237\n",
      "Completed iteration 1330.  Loss: 6.677651358519237\n",
      "Completed iteration 1335.  Loss: 6.677651358519237\n",
      "Completed iteration 1340.  Loss: 6.677651358519237\n",
      "Completed iteration 1345.  Loss: 6.677651358519237\n",
      "Completed iteration 1350.  Loss: 6.677651358519237\n",
      "Completed iteration 1355.  Loss: 6.677651358519237\n",
      "Completed iteration 1360.  Loss: 6.677651358519237\n",
      "Completed iteration 1365.  Loss: 6.677651358519237\n",
      "Completed iteration 1370.  Loss: 6.677651358519237\n",
      "Completed iteration 1375.  Loss: 6.677651358519237\n",
      "Completed iteration 1380.  Loss: 6.677651358519237\n",
      "Completed iteration 1385.  Loss: 6.677651358519237\n",
      "Completed iteration 1390.  Loss: 6.677651358519237\n",
      "Completed iteration 1395.  Loss: 6.677651358519237\n",
      "Completed iteration 1400.  Loss: 6.677651358519237\n",
      "Completed iteration 1405.  Loss: 6.677651358519237\n",
      "Completed iteration 1410.  Loss: 6.677651358519237\n",
      "Completed iteration 1415.  Loss: 6.677651358519237\n",
      "Completed iteration 1420.  Loss: 6.677651358519237\n",
      "Completed iteration 1425.  Loss: 6.677651358519237\n",
      "Completed iteration 1430.  Loss: 6.677651358519237\n",
      "Completed iteration 1435.  Loss: 6.677651358519237\n",
      "Completed iteration 1440.  Loss: 6.677651358519237\n",
      "Completed iteration 1445.  Loss: 6.677651358519237\n",
      "Completed iteration 1450.  Loss: 6.677651358519237\n",
      "Completed iteration 1455.  Loss: 6.677651358519237\n",
      "Completed iteration 1460.  Loss: 6.677651358519237\n",
      "Completed iteration 1465.  Loss: 6.677651358519237\n",
      "Completed iteration 1470.  Loss: 6.677651358519237\n",
      "Completed iteration 1475.  Loss: 6.677651358519237\n",
      "Completed iteration 1480.  Loss: 6.677651358519237\n",
      "Completed iteration 1485.  Loss: 6.677651358519237\n",
      "Completed iteration 1490.  Loss: 6.677651358519237\n",
      "Completed iteration 1495.  Loss: 6.677651358519237\n",
      "Completed iteration 1500.  Loss: 6.677651358519237\n",
      "Completed iteration 1505.  Loss: 6.677651358519237\n",
      "Completed iteration 1510.  Loss: 6.677651358519237\n",
      "Completed iteration 1515.  Loss: 6.677651358519237\n",
      "Completed iteration 1520.  Loss: 6.677651358519237\n",
      "Completed iteration 1525.  Loss: 6.677651358519237\n",
      "Completed iteration 1530.  Loss: 6.677651358519237\n",
      "Completed iteration 1535.  Loss: 6.677651358519237\n",
      "Completed iteration 1540.  Loss: 6.677651358519237\n",
      "Completed iteration 1545.  Loss: 6.677651358519237\n",
      "Completed iteration 1550.  Loss: 6.677651358519237\n",
      "Completed iteration 1555.  Loss: 6.677651358519237\n",
      "Completed iteration 1560.  Loss: 6.677651358519237\n",
      "Completed iteration 1565.  Loss: 6.677651358519237\n",
      "Completed iteration 1570.  Loss: 6.677651358519237\n",
      "Completed iteration 1575.  Loss: 6.677651358519237\n",
      "Completed iteration 1580.  Loss: 6.677651358519237\n",
      "Completed iteration 1585.  Loss: 6.677651358519237\n",
      "Completed iteration 1590.  Loss: 6.677651358519237\n",
      "Completed iteration 1595.  Loss: 6.677651358519237\n",
      "Completed iteration 1600.  Loss: 6.677651358519237\n",
      "Completed iteration 1605.  Loss: 6.677651358519237\n",
      "Completed iteration 1610.  Loss: 6.677651358519237\n",
      "Completed iteration 1615.  Loss: 6.677651358519237\n",
      "Completed iteration 1620.  Loss: 6.677651358519237\n",
      "Completed iteration 1625.  Loss: 6.677651358519237\n",
      "Completed iteration 1630.  Loss: 6.677651358519237\n",
      "Completed iteration 1635.  Loss: 6.677651358519237\n",
      "Completed iteration 1640.  Loss: 6.677651358519237\n",
      "Completed iteration 1645.  Loss: 6.677651358519237\n",
      "Completed iteration 1650.  Loss: 6.677651358519237\n",
      "Completed iteration 1655.  Loss: 6.677651358519237\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed iteration 1660.  Loss: 6.677651358519237\n",
      "Completed iteration 1665.  Loss: 6.677651358519237\n",
      "Completed iteration 1670.  Loss: 6.677651358519237\n",
      "Completed iteration 1675.  Loss: 6.677651358519237\n",
      "Completed iteration 1680.  Loss: 6.677651358519237\n",
      "Completed iteration 1685.  Loss: 6.677651358519237\n",
      "Completed iteration 1690.  Loss: 6.677651358519237\n",
      "Completed iteration 1695.  Loss: 6.677651358519237\n",
      "Completed iteration 1700.  Loss: 6.677651358519237\n",
      "Completed iteration 1705.  Loss: 6.677651358519237\n",
      "Completed iteration 1710.  Loss: 6.677651358519237\n",
      "Completed iteration 1715.  Loss: 6.677651358519237\n",
      "Completed iteration 1720.  Loss: 6.677651358519237\n",
      "Completed iteration 1725.  Loss: 6.677651358519237\n",
      "Completed iteration 1730.  Loss: 6.677651358519237\n",
      "Completed iteration 1735.  Loss: 6.677651358519237\n",
      "Completed iteration 1740.  Loss: 6.677651358519237\n",
      "Completed iteration 1745.  Loss: 6.677651358519237\n",
      "Completed iteration 1750.  Loss: 6.677651358519237\n",
      "Completed iteration 1755.  Loss: 6.677651358519237\n",
      "Completed iteration 1760.  Loss: 6.677651358519237\n",
      "Completed iteration 1765.  Loss: 6.677651358519237\n",
      "Completed iteration 1770.  Loss: 6.677651358519237\n",
      "Completed iteration 1775.  Loss: 6.677651358519237\n",
      "Completed iteration 1780.  Loss: 6.677651358519237\n",
      "Completed iteration 1785.  Loss: 6.677651358519237\n",
      "Completed iteration 1790.  Loss: 6.677651358519237\n",
      "Completed iteration 1795.  Loss: 6.677651358519237\n",
      "Completed iteration 1800.  Loss: 6.677651358519237\n",
      "Completed iteration 1805.  Loss: 6.677651358519237\n",
      "Completed iteration 1810.  Loss: 6.677651358519237\n",
      "Completed iteration 1815.  Loss: 6.677651358519237\n",
      "Completed iteration 1820.  Loss: 6.677651358519237\n",
      "Completed iteration 1825.  Loss: 6.677651358519237\n",
      "Completed iteration 1830.  Loss: 6.677651358519237\n",
      "Completed iteration 1835.  Loss: 6.677651358519237\n",
      "Completed iteration 1840.  Loss: 6.677651358519237\n",
      "Completed iteration 1845.  Loss: 6.677651358519237\n",
      "Completed iteration 1850.  Loss: 6.677651358519237\n",
      "Completed iteration 1855.  Loss: 6.677651358519237\n",
      "Completed iteration 1860.  Loss: 6.677651358519237\n",
      "Completed iteration 1865.  Loss: 6.677651358519237\n",
      "Completed iteration 1870.  Loss: 6.677651358519237\n",
      "Completed iteration 1875.  Loss: 6.677651358519237\n",
      "Completed iteration 1880.  Loss: 6.677651358519237\n",
      "Completed iteration 1885.  Loss: 6.677651358519237\n",
      "Completed iteration 1890.  Loss: 6.677651358519237\n",
      "Completed iteration 1895.  Loss: 6.677651358519237\n",
      "Completed iteration 1900.  Loss: 6.677651358519237\n",
      "Completed iteration 1905.  Loss: 6.677651358519237\n",
      "Completed iteration 1910.  Loss: 6.677651358519237\n",
      "Completed iteration 1915.  Loss: 6.677651358519237\n",
      "Completed iteration 1920.  Loss: 6.677651358519237\n",
      "Completed iteration 1925.  Loss: 6.677651358519237\n",
      "Completed iteration 1930.  Loss: 6.677651358519237\n",
      "Completed iteration 1935.  Loss: 6.677651358519237\n",
      "Completed iteration 1940.  Loss: 6.677651358519237\n",
      "Completed iteration 1945.  Loss: 6.677651358519237\n",
      "Completed iteration 1950.  Loss: 6.677651358519237\n",
      "Completed iteration 1955.  Loss: 6.677651358519237\n",
      "Completed iteration 1960.  Loss: 6.677651358519237\n",
      "Completed iteration 1965.  Loss: 6.677651358519237\n",
      "Completed iteration 1970.  Loss: 6.677651358519237\n",
      "Completed iteration 1975.  Loss: 6.677651358519237\n",
      "Completed iteration 1980.  Loss: 6.677651358519237\n",
      "Completed iteration 1985.  Loss: 6.677651358519237\n",
      "Completed iteration 1990.  Loss: 6.677651358519237\n",
      "Completed iteration 1995.  Loss: 6.677651358519237\n",
      "Completed iteration 2000.  Loss: 6.677651358519237\n",
      "Completed iteration 2005.  Loss: 6.677651358519237\n",
      "Completed iteration 2010.  Loss: 6.677651358519237\n",
      "Completed iteration 2015.  Loss: 6.677651358519237\n",
      "Completed iteration 2020.  Loss: 6.677651358519237\n",
      "Completed iteration 2025.  Loss: 6.677651358519237\n",
      "Completed iteration 2030.  Loss: 6.677651358519237\n",
      "Completed iteration 2035.  Loss: 6.677651358519237\n",
      "Completed iteration 2040.  Loss: 6.677651358519237\n",
      "Completed iteration 2045.  Loss: 6.677651358519237\n",
      "Completed iteration 2050.  Loss: 6.677651358519237\n",
      "Completed iteration 2055.  Loss: 6.677651358519237\n",
      "Completed iteration 2060.  Loss: 6.677651358519237\n",
      "Completed iteration 2065.  Loss: 6.677651358519237\n",
      "Completed iteration 2070.  Loss: 6.677651358519237\n",
      "Completed iteration 2075.  Loss: 6.677651358519237\n",
      "Completed iteration 2080.  Loss: 6.677651358519237\n",
      "Completed iteration 2085.  Loss: 6.677651358519237\n",
      "Completed iteration 2090.  Loss: 6.677651358519237\n",
      "Completed iteration 2095.  Loss: 6.677651358519237\n",
      "Completed iteration 2100.  Loss: 6.677651358519237\n",
      "Completed iteration 2105.  Loss: 6.677651358519237\n",
      "Completed iteration 2110.  Loss: 6.677651358519237\n",
      "Completed iteration 2115.  Loss: 6.677651358519237\n",
      "Completed iteration 2120.  Loss: 6.677651358519237\n",
      "Completed iteration 2125.  Loss: 6.677651358519237\n",
      "Completed iteration 2130.  Loss: 6.677651358519237\n",
      "Completed iteration 2135.  Loss: 6.677651358519237\n",
      "Completed iteration 2140.  Loss: 6.677651358519237\n",
      "Completed iteration 2145.  Loss: 6.677651358519237\n",
      "Completed iteration 2150.  Loss: 6.677651358519237\n",
      "Completed iteration 2155.  Loss: 6.677651358519237\n",
      "Completed iteration 2160.  Loss: 6.677651358519237\n",
      "Completed iteration 2165.  Loss: 6.677651358519237\n",
      "Completed iteration 2170.  Loss: 6.677651358519237\n",
      "Completed iteration 2175.  Loss: 6.677651358519237\n",
      "Completed iteration 2180.  Loss: 6.677651358519237\n",
      "Completed iteration 2185.  Loss: 6.677651358519237\n",
      "Completed iteration 2190.  Loss: 6.677651358519237\n",
      "Completed iteration 2195.  Loss: 6.677651358519237\n",
      "Completed iteration 2200.  Loss: 6.677651358519237\n",
      "Completed iteration 2205.  Loss: 6.677651358519237\n",
      "Completed iteration 2210.  Loss: 6.677651358519237\n",
      "Completed iteration 2215.  Loss: 6.677651358519237\n",
      "Completed iteration 2220.  Loss: 6.677651358519237\n",
      "Completed iteration 2225.  Loss: 6.677651358519237\n",
      "Completed iteration 2230.  Loss: 6.677651358519237\n",
      "Completed iteration 2235.  Loss: 6.677651358519237\n",
      "Completed iteration 2240.  Loss: 6.677651358519237\n",
      "Completed iteration 2245.  Loss: 6.677651358519237\n",
      "Completed iteration 2250.  Loss: 6.677651358519237\n",
      "Completed iteration 2255.  Loss: 6.677651358519237\n",
      "Completed iteration 2260.  Loss: 6.677651358519237\n",
      "Completed iteration 2265.  Loss: 6.677651358519237\n",
      "Completed iteration 2270.  Loss: 6.677651358519237\n",
      "Completed iteration 2275.  Loss: 6.677651358519237\n",
      "Completed iteration 2280.  Loss: 6.677651358519237\n",
      "Completed iteration 2285.  Loss: 6.677651358519237\n",
      "Completed iteration 2290.  Loss: 6.677651358519237\n",
      "Completed iteration 2295.  Loss: 6.677651358519237\n",
      "Completed iteration 2300.  Loss: 6.677651358519237\n",
      "Completed iteration 2305.  Loss: 6.677651358519237\n",
      "Completed iteration 2310.  Loss: 6.677651358519237\n",
      "Completed iteration 2315.  Loss: 6.677651358519237\n",
      "Completed iteration 2320.  Loss: 6.677651358519237\n",
      "Completed iteration 2325.  Loss: 6.677651358519237\n",
      "Completed iteration 2330.  Loss: 6.677651358519237\n",
      "Completed iteration 2335.  Loss: 6.677651358519237\n",
      "Completed iteration 2340.  Loss: 6.677651358519237\n",
      "Completed iteration 2345.  Loss: 6.677651358519237\n",
      "Completed iteration 2350.  Loss: 6.677651358519237\n",
      "Completed iteration 2355.  Loss: 6.677651358519237\n",
      "Completed iteration 2360.  Loss: 6.677651358519237\n",
      "Completed iteration 2365.  Loss: 6.677651358519237\n",
      "Completed iteration 2370.  Loss: 6.677651358519237\n",
      "Completed iteration 2375.  Loss: 6.677651358519237\n",
      "Completed iteration 2380.  Loss: 6.677651358519237\n",
      "Completed iteration 2385.  Loss: 6.677651358519237\n",
      "Completed iteration 2390.  Loss: 6.677651358519237\n",
      "Completed iteration 2395.  Loss: 6.677651358519237\n",
      "Completed iteration 2400.  Loss: 6.677651358519237\n",
      "Completed iteration 2405.  Loss: 6.677651358519237\n",
      "Completed iteration 2410.  Loss: 6.677651358519237\n",
      "Completed iteration 2415.  Loss: 6.677651358519237\n",
      "Completed iteration 2420.  Loss: 6.677651358519237\n",
      "Completed iteration 2425.  Loss: 6.677651358519237\n",
      "Completed iteration 2430.  Loss: 6.677651358519237\n",
      "Completed iteration 2435.  Loss: 6.677651358519237\n",
      "Completed iteration 2440.  Loss: 6.677651358519237\n",
      "Completed iteration 2445.  Loss: 6.677651358519237\n",
      "Completed iteration 2450.  Loss: 6.677651358519237\n",
      "Completed iteration 2455.  Loss: 6.677651358519237\n",
      "Completed iteration 2460.  Loss: 6.677651358519237\n",
      "Completed iteration 2465.  Loss: 6.677651358519237\n",
      "Completed iteration 2470.  Loss: 6.677651358519237\n",
      "Completed iteration 2475.  Loss: 6.677651358519237\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed iteration 2480.  Loss: 6.677651358519237\n",
      "Completed iteration 2485.  Loss: 6.677651358519237\n",
      "Completed iteration 2490.  Loss: 6.677651358519237\n",
      "Completed iteration 2495.  Loss: 6.677651358519237\n",
      "Completed iteration 2500.  Loss: 6.677651358519237\n",
      "Completed iteration 2505.  Loss: 6.677651358519237\n",
      "Completed iteration 2510.  Loss: 6.677651358519237\n",
      "Completed iteration 2515.  Loss: 6.677651358519237\n",
      "Completed iteration 2520.  Loss: 6.677651358519237\n",
      "Completed iteration 2525.  Loss: 6.677651358519237\n",
      "Completed iteration 2530.  Loss: 6.677651358519237\n",
      "Completed iteration 2535.  Loss: 6.677651358519237\n",
      "Completed iteration 2540.  Loss: 6.677651358519237\n",
      "Completed iteration 2545.  Loss: 6.677651358519237\n",
      "Completed iteration 2550.  Loss: 6.677651358519237\n",
      "Completed iteration 2555.  Loss: 6.677651358519237\n",
      "Completed iteration 2560.  Loss: 6.677651358519237\n",
      "Completed iteration 2565.  Loss: 6.677651358519237\n",
      "Completed iteration 2570.  Loss: 6.677651358519237\n",
      "Completed iteration 2575.  Loss: 6.677651358519237\n",
      "Completed iteration 2580.  Loss: 6.677651358519237\n",
      "Completed iteration 2585.  Loss: 6.677651358519237\n",
      "Completed iteration 2590.  Loss: 6.677651358519237\n",
      "Completed iteration 2595.  Loss: 6.677651358519237\n",
      "Completed iteration 2600.  Loss: 6.677651358519237\n",
      "Completed iteration 2605.  Loss: 6.677651358519237\n",
      "Completed iteration 2610.  Loss: 6.677651358519237\n",
      "Completed iteration 2615.  Loss: 6.677651358519237\n",
      "Completed iteration 2620.  Loss: 6.677651358519237\n",
      "Completed iteration 2625.  Loss: 6.677651358519237\n",
      "Completed iteration 2630.  Loss: 6.677651358519237\n",
      "Completed iteration 2635.  Loss: 6.677651358519237\n",
      "Completed iteration 2640.  Loss: 6.677651358519237\n",
      "Completed iteration 2645.  Loss: 6.677651358519237\n",
      "Completed iteration 2650.  Loss: 6.677651358519237\n",
      "Completed iteration 2655.  Loss: 6.677651358519237\n",
      "Completed iteration 2660.  Loss: 6.677651358519237\n",
      "Completed iteration 2665.  Loss: 6.677651358519237\n",
      "Completed iteration 2670.  Loss: 6.677651358519237\n",
      "Completed iteration 2675.  Loss: 6.677651358519237\n",
      "Completed iteration 2680.  Loss: 6.677651358519237\n",
      "Completed iteration 2685.  Loss: 6.677651358519237\n",
      "Completed iteration 2690.  Loss: 6.677651358519237\n",
      "Completed iteration 2695.  Loss: 6.677651358519237\n",
      "Completed iteration 2700.  Loss: 6.677651358519237\n",
      "Completed iteration 2705.  Loss: 6.677651358519237\n",
      "Completed iteration 2710.  Loss: 6.677651358519237\n",
      "Completed iteration 2715.  Loss: 6.677651358519237\n",
      "Completed iteration 2720.  Loss: 6.677651358519237\n",
      "Completed iteration 2725.  Loss: 6.677651358519237\n",
      "Completed iteration 2730.  Loss: 6.677651358519237\n",
      "Completed iteration 2735.  Loss: 6.677651358519237\n",
      "Completed iteration 2740.  Loss: 6.677651358519237\n",
      "Completed iteration 2745.  Loss: 6.677651358519237\n",
      "Completed iteration 2750.  Loss: 6.677651358519237\n",
      "Completed iteration 2755.  Loss: 6.677651358519237\n",
      "Completed iteration 2760.  Loss: 6.677651358519237\n",
      "Completed iteration 2765.  Loss: 6.677651358519237\n",
      "Completed iteration 2770.  Loss: 6.677651358519237\n",
      "Completed iteration 2775.  Loss: 6.677651358519237\n",
      "Completed iteration 2780.  Loss: 6.677651358519237\n",
      "Completed iteration 2785.  Loss: 6.677651358519237\n",
      "Completed iteration 2790.  Loss: 6.677651358519237\n",
      "Completed iteration 2795.  Loss: 6.677651358519237\n",
      "Completed iteration 2800.  Loss: 6.677651358519237\n",
      "Completed iteration 2805.  Loss: 6.677651358519237\n",
      "Completed iteration 2810.  Loss: 6.677651358519237\n",
      "Completed iteration 2815.  Loss: 6.677651358519237\n",
      "Completed iteration 2820.  Loss: 6.677651358519237\n",
      "Completed iteration 2825.  Loss: 6.677651358519237\n",
      "Completed iteration 2830.  Loss: 6.677651358519237\n",
      "Completed iteration 2835.  Loss: 6.677651358519237\n",
      "Completed iteration 2840.  Loss: 6.677651358519237\n",
      "Completed iteration 2845.  Loss: 6.677651358519237\n",
      "Completed iteration 2850.  Loss: 6.677651358519237\n",
      "Completed iteration 2855.  Loss: 6.677651358519237\n",
      "Completed iteration 2860.  Loss: 6.677651358519237\n",
      "Completed iteration 2865.  Loss: 6.677651358519237\n",
      "Completed iteration 2870.  Loss: 6.677651358519237\n",
      "Completed iteration 2875.  Loss: 6.677651358519237\n",
      "Completed iteration 2880.  Loss: 6.677651358519237\n",
      "Completed iteration 2885.  Loss: 6.677651358519237\n",
      "Completed iteration 2890.  Loss: 6.677651358519237\n",
      "Completed iteration 2895.  Loss: 6.677651358519237\n",
      "Completed iteration 2900.  Loss: 6.677651358519237\n",
      "Completed iteration 2905.  Loss: 6.677651358519237\n",
      "Completed iteration 2910.  Loss: 6.677651358519237\n",
      "Completed iteration 2915.  Loss: 6.677651358519237\n",
      "Completed iteration 2920.  Loss: 6.677651358519237\n",
      "Completed iteration 2925.  Loss: 6.677651358519237\n",
      "Completed iteration 2930.  Loss: 6.677651358519237\n",
      "Completed iteration 2935.  Loss: 6.677651358519237\n",
      "Completed iteration 2940.  Loss: 6.677651358519237\n",
      "Completed iteration 2945.  Loss: 6.677651358519237\n",
      "Completed iteration 2950.  Loss: 6.677651358519237\n",
      "Completed iteration 2955.  Loss: 6.677651358519237\n",
      "Completed iteration 2960.  Loss: 6.677651358519237\n",
      "Completed iteration 2965.  Loss: 6.677651358519237\n",
      "Completed iteration 2970.  Loss: 6.677651358519237\n",
      "Completed iteration 2975.  Loss: 6.677651358519237\n",
      "Completed iteration 2980.  Loss: 6.677651358519237\n",
      "Completed iteration 2985.  Loss: 6.677651358519237\n",
      "Completed iteration 2990.  Loss: 6.677651358519237\n",
      "Completed iteration 2995.  Loss: 6.677651358519237\n",
      "Completed iteration 3000.  Loss: 6.677651358519237\n",
      "Completed iteration 3005.  Loss: 6.677651358519237\n",
      "Completed iteration 3010.  Loss: 6.677651358519237\n",
      "Completed iteration 3015.  Loss: 6.677651358519237\n",
      "Completed iteration 3020.  Loss: 6.677651358519237\n",
      "Completed iteration 3025.  Loss: 6.677651358519237\n",
      "Completed iteration 3030.  Loss: 6.677651358519237\n",
      "Completed iteration 3035.  Loss: 6.677651358519237\n",
      "Completed iteration 3040.  Loss: 6.677651358519237\n",
      "Completed iteration 3045.  Loss: 6.677651358519237\n",
      "Completed iteration 3050.  Loss: 6.677651358519237\n",
      "Completed iteration 3055.  Loss: 6.677651358519237\n",
      "Completed iteration 3060.  Loss: 6.677651358519237\n",
      "Completed iteration 3065.  Loss: 6.677651358519237\n",
      "Completed iteration 3070.  Loss: 6.677651358519237\n",
      "Completed iteration 3075.  Loss: 6.677651358519237\n",
      "Completed iteration 3080.  Loss: 6.677651358519237\n",
      "Completed iteration 3085.  Loss: 6.677651358519237\n",
      "Completed iteration 3090.  Loss: 6.677651358519237\n",
      "Completed iteration 3095.  Loss: 6.677651358519237\n",
      "Completed iteration 3100.  Loss: 6.677651358519237\n",
      "Completed iteration 3105.  Loss: 6.677651358519237\n",
      "Completed iteration 3110.  Loss: 6.677651358519237\n",
      "Completed iteration 3115.  Loss: 6.677651358519237\n",
      "Completed iteration 3120.  Loss: 6.677651358519237\n",
      "Completed iteration 3125.  Loss: 6.677651358519237\n",
      "Completed iteration 3130.  Loss: 6.677651358519237\n",
      "Completed iteration 3135.  Loss: 6.677651358519237\n",
      "Completed iteration 3140.  Loss: 6.677651358519237\n",
      "Completed iteration 3145.  Loss: 6.677651358519237\n",
      "Completed iteration 3150.  Loss: 6.677651358519237\n",
      "Completed iteration 3155.  Loss: 6.677651358519237\n",
      "Completed iteration 3160.  Loss: 6.677651358519237\n",
      "Completed iteration 3165.  Loss: 6.677651358519237\n",
      "Completed iteration 3170.  Loss: 6.677651358519237\n",
      "Completed iteration 3175.  Loss: 6.677651358519237\n",
      "Completed iteration 3180.  Loss: 6.677651358519237\n",
      "Completed iteration 3185.  Loss: 6.677651358519237\n",
      "Completed iteration 3190.  Loss: 6.677651358519237\n",
      "Completed iteration 3195.  Loss: 6.677651358519237\n",
      "Completed iteration 3200.  Loss: 6.677651358519237\n",
      "Completed iteration 3205.  Loss: 6.677651358519237\n",
      "Completed iteration 3210.  Loss: 6.677651358519237\n",
      "Completed iteration 3215.  Loss: 6.677651358519237\n",
      "Completed iteration 3220.  Loss: 6.677651358519237\n",
      "Completed iteration 3225.  Loss: 6.677651358519237\n",
      "Completed iteration 3230.  Loss: 6.677651358519237\n",
      "Completed iteration 3235.  Loss: 6.677651358519237\n",
      "Completed iteration 3240.  Loss: 6.677651358519237\n",
      "Completed iteration 3245.  Loss: 6.677651358519237\n",
      "Completed iteration 3250.  Loss: 6.677651358519237\n",
      "Completed iteration 3255.  Loss: 6.677651358519237\n",
      "Completed iteration 3260.  Loss: 6.677651358519237\n",
      "Completed iteration 3265.  Loss: 6.677651358519237\n",
      "Completed iteration 3270.  Loss: 6.677651358519237\n",
      "Completed iteration 3275.  Loss: 6.677651358519237\n",
      "Completed iteration 3280.  Loss: 6.677651358519237\n",
      "Completed iteration 3285.  Loss: 6.677651358519237\n",
      "Completed iteration 3290.  Loss: 6.677651358519237\n",
      "Completed iteration 3295.  Loss: 6.677651358519237\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed iteration 3300.  Loss: 6.677651358519237\n",
      "Completed iteration 3305.  Loss: 6.677651358519237\n",
      "Completed iteration 3310.  Loss: 6.677651358519237\n",
      "Completed iteration 3315.  Loss: 6.677651358519237\n",
      "Completed iteration 3320.  Loss: 6.677651358519237\n",
      "Completed iteration 3325.  Loss: 6.677651358519237\n",
      "Completed iteration 3330.  Loss: 6.677651358519237\n",
      "Completed iteration 3335.  Loss: 6.677651358519237\n",
      "Completed iteration 3340.  Loss: 6.677651358519237\n",
      "Completed iteration 3345.  Loss: 6.677651358519237\n",
      "Completed iteration 3350.  Loss: 6.677651358519237\n",
      "Completed iteration 3355.  Loss: 6.677651358519237\n",
      "Completed iteration 3360.  Loss: 6.677651358519237\n",
      "Completed iteration 3365.  Loss: 6.677651358519237\n",
      "Completed iteration 3370.  Loss: 6.677651358519237\n",
      "Completed iteration 3375.  Loss: 6.677651358519237\n",
      "Completed iteration 3380.  Loss: 6.677651358519237\n",
      "Completed iteration 3385.  Loss: 6.677651358519237\n",
      "Completed iteration 3390.  Loss: 6.677651358519237\n",
      "Completed iteration 3395.  Loss: 6.677651358519237\n",
      "Completed iteration 3400.  Loss: 6.677651358519237\n",
      "Completed iteration 3405.  Loss: 6.677651358519237\n",
      "Completed iteration 3410.  Loss: 6.677651358519237\n",
      "Completed iteration 3415.  Loss: 6.677651358519237\n",
      "Completed iteration 3420.  Loss: 6.677651358519237\n",
      "Completed iteration 3425.  Loss: 6.677651358519237\n",
      "Completed iteration 3430.  Loss: 6.677651358519237\n",
      "Completed iteration 3435.  Loss: 6.677651358519237\n",
      "Completed iteration 3440.  Loss: 6.677651358519237\n",
      "Completed iteration 3445.  Loss: 6.677651358519237\n",
      "Completed iteration 3450.  Loss: 6.677651358519237\n",
      "Completed iteration 3455.  Loss: 6.677651358519237\n",
      "Completed iteration 3460.  Loss: 6.677651358519237\n",
      "Completed iteration 3465.  Loss: 6.677651358519237\n",
      "Completed iteration 3470.  Loss: 6.677651358519237\n",
      "Completed iteration 3475.  Loss: 6.677651358519237\n",
      "Completed iteration 3480.  Loss: 6.677651358519237\n",
      "Completed iteration 3485.  Loss: 6.677651358519237\n",
      "Completed iteration 3490.  Loss: 6.677651358519237\n",
      "Completed iteration 3495.  Loss: 6.677651358519237\n",
      "Completed iteration 3500.  Loss: 6.677651358519237\n",
      "Completed iteration 3505.  Loss: 6.677651358519237\n",
      "Completed iteration 3510.  Loss: 6.677651358519237\n",
      "Completed iteration 3515.  Loss: 6.677651358519237\n",
      "Completed iteration 3520.  Loss: 6.677651358519237\n",
      "Completed iteration 3525.  Loss: 6.677651358519237\n",
      "Completed iteration 3530.  Loss: 6.677651358519237\n",
      "Completed iteration 3535.  Loss: 6.677651358519237\n",
      "Completed iteration 3540.  Loss: 6.677651358519237\n",
      "Completed iteration 3545.  Loss: 6.677651358519237\n",
      "Completed iteration 3550.  Loss: 6.677651358519237\n",
      "Completed iteration 3555.  Loss: 6.677651358519237\n",
      "Completed iteration 3560.  Loss: 6.677651358519237\n",
      "Completed iteration 3565.  Loss: 6.677651358519237\n",
      "Completed iteration 3570.  Loss: 6.677651358519237\n",
      "Completed iteration 3575.  Loss: 6.677651358519237\n",
      "Completed iteration 3580.  Loss: 6.677651358519237\n",
      "Completed iteration 3585.  Loss: 6.677651358519237\n",
      "Completed iteration 3590.  Loss: 6.677651358519237\n",
      "Completed iteration 3595.  Loss: 6.677651358519237\n",
      "Completed iteration 3600.  Loss: 6.677651358519237\n",
      "Completed iteration 3605.  Loss: 6.677651358519237\n",
      "Completed iteration 3610.  Loss: 6.677651358519237\n",
      "Completed iteration 3615.  Loss: 6.677651358519237\n",
      "Completed iteration 3620.  Loss: 6.677651358519237\n",
      "Completed iteration 3625.  Loss: 6.677651358519237\n",
      "Completed iteration 3630.  Loss: 6.677651358519237\n",
      "Completed iteration 3635.  Loss: 6.677651358519237\n",
      "Completed iteration 3640.  Loss: 6.677651358519237\n",
      "Completed iteration 3645.  Loss: 6.677651358519237\n",
      "Completed iteration 3650.  Loss: 6.677651358519237\n",
      "Completed iteration 3655.  Loss: 6.677651358519237\n",
      "Completed iteration 3660.  Loss: 6.677651358519237\n",
      "Completed iteration 3665.  Loss: 6.677651358519237\n",
      "Completed iteration 3670.  Loss: 6.677651358519237\n",
      "Completed iteration 3675.  Loss: 6.677651358519237\n",
      "Completed iteration 3680.  Loss: 6.677651358519237\n",
      "Completed iteration 3685.  Loss: 6.677651358519237\n",
      "Completed iteration 3690.  Loss: 6.677651358519237\n",
      "Completed iteration 3695.  Loss: 6.677651358519237\n",
      "Completed iteration 3700.  Loss: 6.677651358519237\n",
      "Completed iteration 3705.  Loss: 6.677651358519237\n",
      "Completed iteration 3710.  Loss: 6.677651358519237\n",
      "Completed iteration 3715.  Loss: 6.677651358519237\n",
      "Completed iteration 3720.  Loss: 6.677651358519237\n",
      "Completed iteration 3725.  Loss: 6.677651358519237\n",
      "Completed iteration 3730.  Loss: 6.677651358519237\n",
      "Completed iteration 3735.  Loss: 6.677651358519237\n",
      "Completed iteration 3740.  Loss: 6.677651358519237\n",
      "Completed iteration 3745.  Loss: 6.677651358519237\n",
      "Completed iteration 3750.  Loss: 6.677651358519237\n",
      "Completed iteration 3755.  Loss: 6.677651358519237\n",
      "Completed iteration 3760.  Loss: 6.677651358519237\n",
      "Completed iteration 3765.  Loss: 6.677651358519237\n",
      "Completed iteration 3770.  Loss: 6.677651358519237\n",
      "Completed iteration 3775.  Loss: 6.677651358519237\n",
      "Completed iteration 3780.  Loss: 6.677651358519237\n",
      "Completed iteration 3785.  Loss: 6.677651358519237\n",
      "Completed iteration 3790.  Loss: 6.677651358519237\n",
      "Completed iteration 3795.  Loss: 6.677651358519237\n",
      "Completed iteration 3800.  Loss: 6.677651358519237\n",
      "Completed iteration 3805.  Loss: 6.677651358519237\n",
      "Completed iteration 3810.  Loss: 6.677651358519237\n",
      "Completed iteration 3815.  Loss: 6.677651358519237\n",
      "Completed iteration 3820.  Loss: 6.677651358519237\n",
      "Completed iteration 3825.  Loss: 6.677651358519237\n",
      "Completed iteration 3830.  Loss: 6.677651358519237\n",
      "Completed iteration 3835.  Loss: 6.677651358519237\n",
      "Completed iteration 3840.  Loss: 6.677651358519237\n",
      "Completed iteration 3845.  Loss: 6.677651358519237\n",
      "Completed iteration 3850.  Loss: 6.677651358519237\n",
      "Completed iteration 3855.  Loss: 6.677651358519237\n",
      "Completed iteration 3860.  Loss: 6.677651358519237\n",
      "Completed iteration 3865.  Loss: 6.677651358519237\n",
      "Completed iteration 3870.  Loss: 6.677651358519237\n",
      "Completed iteration 3875.  Loss: 6.677651358519237\n",
      "Completed iteration 3880.  Loss: 6.677651358519237\n",
      "Completed iteration 3885.  Loss: 6.677651358519237\n",
      "Completed iteration 3890.  Loss: 6.677651358519237\n",
      "Completed iteration 3895.  Loss: 6.677651358519237\n",
      "Completed iteration 3900.  Loss: 6.677651358519237\n",
      "Completed iteration 3905.  Loss: 6.677651358519237\n",
      "Completed iteration 3910.  Loss: 6.677651358519237\n",
      "Completed iteration 3915.  Loss: 6.677651358519237\n",
      "Completed iteration 3920.  Loss: 6.677651358519237\n",
      "Completed iteration 3925.  Loss: 6.677651358519237\n",
      "Completed iteration 3930.  Loss: 6.677651358519237\n",
      "Completed iteration 3935.  Loss: 6.677651358519237\n",
      "Completed iteration 3940.  Loss: 6.677651358519237\n",
      "Completed iteration 3945.  Loss: 6.677651358519237\n",
      "Completed iteration 3950.  Loss: 6.677651358519237\n",
      "Completed iteration 3955.  Loss: 6.677651358519237\n",
      "Completed iteration 3960.  Loss: 6.677651358519237\n",
      "Completed iteration 3965.  Loss: 6.677651358519237\n",
      "Completed iteration 3970.  Loss: 6.677651358519237\n",
      "Completed iteration 3975.  Loss: 6.677651358519237\n",
      "Completed iteration 3980.  Loss: 6.677651358519237\n",
      "Completed iteration 3985.  Loss: 6.677651358519237\n",
      "Completed iteration 3990.  Loss: 6.677651358519237\n",
      "Completed iteration 3995.  Loss: 6.677651358519237\n",
      "Completed iteration 4000.  Loss: 6.677651358519237\n",
      "Completed iteration 4005.  Loss: 6.677651358519237\n",
      "Completed iteration 4010.  Loss: 6.677651358519237\n",
      "Completed iteration 4015.  Loss: 6.677651358519237\n",
      "Completed iteration 4020.  Loss: 6.677651358519237\n",
      "Completed iteration 4025.  Loss: 6.677651358519237\n",
      "Completed iteration 4030.  Loss: 6.677651358519237\n",
      "Completed iteration 4035.  Loss: 6.677651358519237\n",
      "Completed iteration 4040.  Loss: 6.677651358519237\n",
      "Completed iteration 4045.  Loss: 6.677651358519237\n",
      "Completed iteration 4050.  Loss: 6.677651358519237\n",
      "Completed iteration 4055.  Loss: 6.677651358519237\n",
      "Completed iteration 4060.  Loss: 6.677651358519237\n",
      "Completed iteration 4065.  Loss: 6.677651358519237\n",
      "Completed iteration 4070.  Loss: 6.677651358519237\n",
      "Completed iteration 4075.  Loss: 6.677651358519237\n",
      "Completed iteration 4080.  Loss: 6.677651358519237\n",
      "Completed iteration 4085.  Loss: 6.677651358519237\n",
      "Completed iteration 4090.  Loss: 6.677651358519237\n",
      "Completed iteration 4095.  Loss: 6.677651358519237\n",
      "Completed iteration 4100.  Loss: 6.677651358519237\n",
      "Completed iteration 4105.  Loss: 6.677651358519237\n",
      "Completed iteration 4110.  Loss: 6.677651358519237\n",
      "Completed iteration 4115.  Loss: 6.677651358519237\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed iteration 4120.  Loss: 6.677651358519237\n",
      "Completed iteration 4125.  Loss: 6.677651358519237\n",
      "Completed iteration 4130.  Loss: 6.677651358519237\n",
      "Completed iteration 4135.  Loss: 6.677651358519237\n",
      "Completed iteration 4140.  Loss: 6.677651358519237\n",
      "Completed iteration 4145.  Loss: 6.677651358519237\n",
      "Completed iteration 4150.  Loss: 6.677651358519237\n",
      "Completed iteration 4155.  Loss: 6.677651358519237\n",
      "Completed iteration 4160.  Loss: 6.677651358519237\n",
      "Completed iteration 4165.  Loss: 6.677651358519237\n",
      "Completed iteration 4170.  Loss: 6.677651358519237\n",
      "Completed iteration 4175.  Loss: 6.677651358519237\n",
      "Completed iteration 4180.  Loss: 6.677651358519237\n",
      "Completed iteration 4185.  Loss: 6.677651358519237\n",
      "Completed iteration 4190.  Loss: 6.677651358519237\n",
      "Completed iteration 4195.  Loss: 6.677651358519237\n",
      "Completed iteration 4200.  Loss: 6.677651358519237\n",
      "Completed iteration 4205.  Loss: 6.677651358519237\n",
      "Completed iteration 4210.  Loss: 6.677651358519237\n",
      "Completed iteration 4215.  Loss: 6.677651358519237\n",
      "Completed iteration 4220.  Loss: 6.677651358519237\n",
      "Completed iteration 4225.  Loss: 6.677651358519237\n",
      "Completed iteration 4230.  Loss: 6.677651358519237\n",
      "Completed iteration 4235.  Loss: 6.677651358519237\n",
      "Completed iteration 4240.  Loss: 6.677651358519237\n",
      "Completed iteration 4245.  Loss: 6.677651358519237\n",
      "Completed iteration 4250.  Loss: 6.677651358519237\n",
      "Completed iteration 4255.  Loss: 6.677651358519237\n",
      "Completed iteration 4260.  Loss: 6.677651358519237\n",
      "Completed iteration 4265.  Loss: 6.677651358519237\n",
      "Completed iteration 4270.  Loss: 6.677651358519237\n",
      "Completed iteration 4275.  Loss: 6.677651358519237\n",
      "Completed iteration 4280.  Loss: 6.677651358519237\n",
      "Completed iteration 4285.  Loss: 6.677651358519237\n",
      "Completed iteration 4290.  Loss: 6.677651358519237\n",
      "Completed iteration 4295.  Loss: 6.677651358519237\n",
      "Completed iteration 4300.  Loss: 6.677651358519237\n",
      "Completed iteration 4305.  Loss: 6.677651358519237\n",
      "Completed iteration 4310.  Loss: 6.677651358519237\n",
      "Completed iteration 4315.  Loss: 6.677651358519237\n",
      "Completed iteration 4320.  Loss: 6.677651358519237\n",
      "Completed iteration 4325.  Loss: 6.677651358519237\n",
      "Completed iteration 4330.  Loss: 6.677651358519237\n",
      "Completed iteration 4335.  Loss: 6.677651358519237\n",
      "Completed iteration 4340.  Loss: 6.677651358519237\n",
      "Completed iteration 4345.  Loss: 6.677651358519237\n",
      "Completed iteration 4350.  Loss: 6.677651358519237\n",
      "Completed iteration 4355.  Loss: 6.677651358519237\n",
      "Completed iteration 4360.  Loss: 6.677651358519237\n",
      "Completed iteration 4365.  Loss: 6.677651358519237\n",
      "Completed iteration 4370.  Loss: 6.677651358519237\n",
      "Completed iteration 4375.  Loss: 6.677651358519237\n",
      "Completed iteration 4380.  Loss: 6.677651358519237\n",
      "Completed iteration 4385.  Loss: 6.677651358519237\n",
      "Completed iteration 4390.  Loss: 6.677651358519237\n",
      "Completed iteration 4395.  Loss: 6.677651358519237\n",
      "Completed iteration 4400.  Loss: 6.677651358519237\n",
      "Completed iteration 4405.  Loss: 6.677651358519237\n",
      "Completed iteration 4410.  Loss: 6.677651358519237\n",
      "Completed iteration 4415.  Loss: 6.677651358519237\n",
      "Completed iteration 4420.  Loss: 6.677651358519237\n",
      "Completed iteration 4425.  Loss: 6.677651358519237\n",
      "Completed iteration 4430.  Loss: 6.677651358519237\n",
      "Completed iteration 4435.  Loss: 6.677651358519237\n",
      "Completed iteration 4440.  Loss: 6.677651358519237\n",
      "Completed iteration 4445.  Loss: 6.677651358519237\n",
      "Completed iteration 4450.  Loss: 6.677651358519237\n",
      "Completed iteration 4455.  Loss: 6.677651358519237\n",
      "Completed iteration 4460.  Loss: 6.677651358519237\n",
      "Completed iteration 4465.  Loss: 6.677651358519237\n",
      "Completed iteration 4470.  Loss: 6.677651358519237\n",
      "Completed iteration 4475.  Loss: 6.677651358519237\n",
      "Completed iteration 4480.  Loss: 6.677651358519237\n",
      "Completed iteration 4485.  Loss: 6.677651358519237\n",
      "Completed iteration 4490.  Loss: 6.677651358519237\n",
      "Completed iteration 4495.  Loss: 6.677651358519237\n",
      "Completed iteration 4500.  Loss: 6.677651358519237\n",
      "Completed iteration 4505.  Loss: 6.677651358519237\n",
      "Completed iteration 4510.  Loss: 6.677651358519237\n",
      "Completed iteration 4515.  Loss: 6.677651358519237\n",
      "Completed iteration 4520.  Loss: 6.677651358519237\n",
      "Completed iteration 4525.  Loss: 6.677651358519237\n",
      "Completed iteration 4530.  Loss: 6.677651358519237\n",
      "Completed iteration 4535.  Loss: 6.677651358519237\n",
      "Completed iteration 4540.  Loss: 6.677651358519237\n",
      "Completed iteration 4545.  Loss: 6.677651358519237\n",
      "Completed iteration 4550.  Loss: 6.677651358519237\n",
      "Completed iteration 4555.  Loss: 6.677651358519237\n",
      "Completed iteration 4560.  Loss: 6.677651358519237\n",
      "Completed iteration 4565.  Loss: 6.677651358519237\n",
      "Completed iteration 4570.  Loss: 6.677651358519237\n",
      "Completed iteration 4575.  Loss: 6.677651358519237\n",
      "Completed iteration 4580.  Loss: 6.677651358519237\n",
      "Completed iteration 4585.  Loss: 6.677651358519237\n",
      "Completed iteration 4590.  Loss: 6.677651358519237\n",
      "Completed iteration 4595.  Loss: 6.677651358519237\n",
      "Completed iteration 4600.  Loss: 6.677651358519237\n",
      "Completed iteration 4605.  Loss: 6.677651358519237\n",
      "Completed iteration 4610.  Loss: 6.677651358519237\n",
      "Completed iteration 4615.  Loss: 6.677651358519237\n",
      "Completed iteration 4620.  Loss: 6.677651358519237\n",
      "Completed iteration 4625.  Loss: 6.677651358519237\n",
      "Completed iteration 4630.  Loss: 6.677651358519237\n",
      "Completed iteration 4635.  Loss: 6.677651358519237\n",
      "Completed iteration 4640.  Loss: 6.677651358519237\n",
      "Completed iteration 4645.  Loss: 6.677651358519237\n",
      "Completed iteration 4650.  Loss: 6.677651358519237\n",
      "Completed iteration 4655.  Loss: 6.677651358519237\n",
      "Completed iteration 4660.  Loss: 6.677651358519237\n",
      "Completed iteration 4665.  Loss: 6.677651358519237\n",
      "Completed iteration 4670.  Loss: 6.677651358519237\n",
      "Completed iteration 4675.  Loss: 6.677651358519237\n",
      "Completed iteration 4680.  Loss: 6.677651358519237\n",
      "Completed iteration 4685.  Loss: 6.677651358519237\n",
      "Completed iteration 4690.  Loss: 6.677651358519237\n",
      "Completed iteration 4695.  Loss: 6.677651358519237\n",
      "Completed iteration 4700.  Loss: 6.677651358519237\n",
      "Completed iteration 4705.  Loss: 6.677651358519237\n",
      "Completed iteration 4710.  Loss: 6.677651358519237\n",
      "Completed iteration 4715.  Loss: 6.677651358519237\n",
      "Completed iteration 4720.  Loss: 6.677651358519237\n",
      "Completed iteration 4725.  Loss: 6.677651358519237\n",
      "Completed iteration 4730.  Loss: 6.677651358519237\n",
      "Completed iteration 4735.  Loss: 6.677651358519237\n",
      "Completed iteration 4740.  Loss: 6.677651358519237\n",
      "Completed iteration 4745.  Loss: 6.677651358519237\n",
      "Completed iteration 4750.  Loss: 6.677651358519237\n",
      "Completed iteration 4755.  Loss: 6.677651358519237\n",
      "Completed iteration 4760.  Loss: 6.677651358519237\n",
      "Completed iteration 4765.  Loss: 6.677651358519237\n",
      "Completed iteration 4770.  Loss: 6.677651358519237\n",
      "Completed iteration 4775.  Loss: 6.677651358519237\n",
      "Completed iteration 4780.  Loss: 6.677651358519237\n",
      "Completed iteration 4785.  Loss: 6.677651358519237\n",
      "Completed iteration 4790.  Loss: 6.677651358519237\n",
      "Completed iteration 4795.  Loss: 6.677651358519237\n",
      "Completed iteration 4800.  Loss: 6.677651358519237\n",
      "Completed iteration 4805.  Loss: 6.677651358519237\n",
      "Completed iteration 4810.  Loss: 6.677651358519237\n",
      "Completed iteration 4815.  Loss: 6.677651358519237\n",
      "Completed iteration 4820.  Loss: 6.677651358519237\n",
      "Completed iteration 4825.  Loss: 6.677651358519237\n",
      "Completed iteration 4830.  Loss: 6.677651358519237\n",
      "Completed iteration 4835.  Loss: 6.677651358519237\n",
      "Completed iteration 4840.  Loss: 6.677651358519237\n",
      "Completed iteration 4845.  Loss: 6.677651358519237\n",
      "Completed iteration 4850.  Loss: 6.677651358519237\n",
      "Completed iteration 4855.  Loss: 6.677651358519237\n",
      "Completed iteration 4860.  Loss: 6.677651358519237\n",
      "Completed iteration 4865.  Loss: 6.677651358519237\n",
      "Completed iteration 4870.  Loss: 6.677651358519237\n",
      "Completed iteration 4875.  Loss: 6.677651358519237\n",
      "Completed iteration 4880.  Loss: 6.677651358519237\n",
      "Completed iteration 4885.  Loss: 6.677651358519237\n",
      "Completed iteration 4890.  Loss: 6.677651358519237\n",
      "Completed iteration 4895.  Loss: 6.677651358519237\n",
      "Completed iteration 4900.  Loss: 6.677651358519237\n",
      "Completed iteration 4905.  Loss: 6.677651358519237\n",
      "Completed iteration 4910.  Loss: 6.677651358519237\n",
      "Completed iteration 4915.  Loss: 6.677651358519237\n",
      "Completed iteration 4920.  Loss: 6.677651358519237\n",
      "Completed iteration 4925.  Loss: 6.677651358519237\n",
      "Completed iteration 4930.  Loss: 6.677651358519237\n",
      "Completed iteration 4935.  Loss: 6.677651358519237\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed iteration 4940.  Loss: 6.677651358519237\n",
      "Completed iteration 4945.  Loss: 6.677651358519237\n",
      "Completed iteration 4950.  Loss: 6.677651358519237\n",
      "Completed iteration 4955.  Loss: 6.677651358519237\n",
      "Completed iteration 4960.  Loss: 6.677651358519237\n",
      "Completed iteration 4965.  Loss: 6.677651358519237\n",
      "Completed iteration 4970.  Loss: 6.677651358519237\n",
      "Completed iteration 4975.  Loss: 6.677651358519237\n",
      "Completed iteration 4980.  Loss: 6.677651358519237\n",
      "Completed iteration 4985.  Loss: 6.677651358519237\n",
      "Completed iteration 4990.  Loss: 6.677651358519237\n",
      "Completed iteration 4995.  Loss: 6.677651358519237\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "6.677651358519237"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn import preprocessing\n",
    "\n",
    "DERIVATIVE_OFFSET = 0.051\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "iris_x = preprocessing.scale(iris[\"data\"])\n",
    "iris_y = iris[\"target\"]\n",
    "Y2 = (iris_y == 0).astype(int)\n",
    "\n",
    "structure = [{\"size\": 3, \"activation\": relu_activation}, \n",
    "             {\"size\": 1, \"activation\": sigmoid_activation}]\n",
    "\n",
    "model = Model(iris_x, Y2, structure, binary_loss_function, learning_rate=0.001)\n",
    "model.train()\n",
    "\n",
    "\n",
    "\n",
    "  \n",
    "        \n",
    "    \n",
    "\n",
    "\n",
    "       \n",
    "        \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
