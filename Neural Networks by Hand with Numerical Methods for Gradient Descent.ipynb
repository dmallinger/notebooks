{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intuitive Neural Networks by Hand\n",
    "\n",
    "This notebook shows how to build a neural network by hand.  The motivation for creating _yet another_ neural network from scratch post is that I believe most get lost in the calculus and lose the concepts.  Additionally, I believe the demonstation code is often designed to be efficient with numpy, not illustrative.\n",
    "\n",
    "Below, we create classes to represent our neural network and then use it to fit to a simple example from the Iris dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Concepts\n",
    "\n",
    "Neural networks are essentially two things:\n",
    "1. A collection of node layers with connecting weights\n",
    "1. A bidirectional algorithm for optimizing those weights with the goal of minimizing a cost function.\n",
    "\n",
    "The bidirectional algorithm goes forwards and backwards (and has analogies to forwards-backwards algorithms in statistics):\n",
    "\n",
    "1. A forward step computes the current outcome probabilities.\n",
    "1. A backwards step updates the weights used (via gradient descent) to minimize the cost function.\n",
    "\n",
    "The bidirectional process iterates (hopefully) to convergence.  After completing the training of the model, making predictions is simply running the forward step.\n",
    "\n",
    "Some ideas may be helpful keeping in mind:\n",
    "* Neural networks are black boxes that mindlessly update parameters.  Our model only has weights as parameters, so these will be \"mindlessly\" tuned.\n",
    "* Gradient descent is an iterative algorithm.  It makes small changes in parameters without attempting to reach the best answer all at once.\n",
    "* Even though neural networks are non-parametric, they perform better with normalized data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"static/neural-network.png\" alt=\"drawing\" width=\"500\"/>\n",
    "\n",
    "A key concept is that neural network weights are linear, with each node receiving a linear combination of the node outputs from the prior layer. For the first layer, nodes receive a linear combination of the dataset features.  For subsequent layers, nodes receive a linear combination of the activation functions of the previous nodes.  In this way, neural networks continually augment the data with linear combinations (followed by non-linear activation functions), projecting iterations of data into new dimensions until a good prediction is made.  As layers get arbitrarily large, one can imagine how these projections capture latent features.\n",
    "\n",
    "Now notice that the weights between these layers are simply edges on a bipartite graph.  This gives us our insight that we can represent the weights as an adjacency matrix between the current layer and the prior layer:\n",
    "\n",
    "<img src=\"static/bipartite-graph.png\" alt=\"drawing\" width=\"500\"/>\n",
    "\n",
    "Thus it follows that weighting the input is simply a matrix multiplication by the adjacency matrix: _W*input + b_."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Layer (Incomplete Version)\n",
    "\n",
    "The first thing that our neural network will need is a notion of a layer.  As with any coding project, we do not know our final state API *yet*, but we have a notion of how to start.  Namely, our Layer will need a size (number of nodes), an activation function, weights, and betas.  We also use a locator pattern to store to which model iteration this Layer belongs (note that we have not defined model iterations yet!).\n",
    "\n",
    "The IncompleteLayer class knows how to apply weights using our bipartite graph insight.  We simply multiply the weights by the input matrix and add our beta terms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class IncompleteLayer:\n",
    "    def __init__(self, model_iteration, size, activation_function, weights, betas):\n",
    "        self.model_iteration = model_iteration  # locator pattern\n",
    "        self.size = size\n",
    "        self.activation_function = activation_function\n",
    "        self.weights = weights\n",
    "        self.betas = betas\n",
    "\n",
    "    def apply_weights(self, layer_input):\n",
    "        \"\"\"Equivalent to a forward propegation on a single layer\"\"\"\n",
    "        Z = np.dot(self.weights, layer_input) + self.betas\n",
    "        output = self.activation_function(Z)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Iteration\n",
    "\n",
    "Every neural network model runs for multiple iterations and these iterations have their own layers, weights, etc.  Thus it makes for a good class to build.\n",
    "\n",
    "Our ModelIteration class should have a `feed_forward` function that runs our `Layer.apply_weights` and then returns the last value (the output of the last layer is the neural network output).  Similarly, predict is simply a call to `feed_forward` under this methodology.\n",
    "\n",
    "We will also need a way to do backprogegation.  The derivative calculations can be passed to Layers.  (Note that we have implicitly defined `Layer.update_weights`.)\n",
    "\n",
    "### Setting Weights\n",
    "\n",
    "Despite defining much of the model iterations and layers, we never wrote any code that sets the values of weights. \n",
    "\n",
    "**On any forward feeding of a model, the weights are defined to be either random initialization or the weights from the prior backpropegation.** Thus ModelIteration will need to loop to create the layers, each time looking at the prior iteration values:\n",
    "\n",
    "```\n",
    "for layer in model_structure:\n",
    "    if prior_iteration:\n",
    "        weights = prior_iteration.weights\n",
    "        betas = prior_iteration.betas\n",
    "    else:\n",
    "        weights = random_initialization()\n",
    "        betas = zeros()\n",
    "```\n",
    "\n",
    "A final wrinkle: To initialize the weights on the first iteration, we must be able to define the dimensions of the weight matrix.  From our bipartite graph, the matrix dimensions will be *(# current layer nodes, # prior layer nodes)*.  However, remember that the first layer has *# dataset features* columns as there is no prior layer.  The `__init__` method of the ModelIteration class updates the pseudocode to include these dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class ModelIteration:\n",
    "    def __init__(self, model, data, Y, learning_rate, prior_iteration=None):\n",
    "        self.model = model  # locator pattern\n",
    "        self.learning_rate = learning_rate\n",
    "        self.prior_iteration = prior_iteration\n",
    "        self.layers = []\n",
    "        \n",
    "        # iterate over each layer to initialize weights, per the above description.\n",
    "        for layer_number, layer in enumerate(self.model.model_structure):\n",
    "            if self.prior_iteration is None: # first iteration, must initialize weights\n",
    "                if 0 == layer_number:\n",
    "                    prior_layer_size = data.shape[1]\n",
    "                else:\n",
    "                    prior_layer_size = self.layers[-1].size\n",
    "                weights = np.random.randn(layer[\"size\"], prior_layer_size) / (layer[\"size\"])**2\n",
    "                betas = np.zeros((layer[\"size\"], 1))\n",
    "            else:\n",
    "                weights = self.prior_iteration.layers[layer_number].weights # backprop output\n",
    "                betas = self.prior_iteration.layers[layer_number].betas            \n",
    "           \n",
    "            layer = Layer(self, layer[\"size\"], layer[\"activation\"], weights, betas)\n",
    "            self.layers.append(layer)\n",
    "        \n",
    "    def feed_forward(self, data):\n",
    "        prior_output = data.T\n",
    "        for layer in self.layers:\n",
    "            output = layer.apply_weights(prior_output)\n",
    "            prior_output = output\n",
    "        return output\n",
    "    \n",
    "    def predict(self, data):\n",
    "         return self.feed_forward(data)\n",
    "        \n",
    "    def evaluate(self, data=None, Y=None):\n",
    "        if data is None:\n",
    "            data = self.model.data\n",
    "            Y = self.model.Y\n",
    "        predictions = self.predict(data)\n",
    "        return self.model.cost_function(predictions, Y)\n",
    "\n",
    "    def propegate_backward(self):\n",
    "        derivatives_list = []\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            derivatives = layer.calculate_derivatives()\n",
    "            derivatives_list.append(derivatives)\n",
    "            \n",
    "        for i, layer in enumerate(self.layers):\n",
    "            layer.update_derivatives(derivatives_list[i])\n",
    "            layer.update_weights(self.learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Class\n",
    "\n",
    "Our model class is effectively a wrapper for ModelIteration that calls the last iteration for `predict`.  The only difference is that our Model class needs a `train` method.  `train` will loop over model iterations, feed forward and back propegate, printing status along the way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class Model:\n",
    "    def __init__(self, data, Y, model_structure, cost_function, learning_rate):\n",
    "        self.data = data\n",
    "        self.Y = Y\n",
    "        self.model_structure = model_structure\n",
    "        self.cost_function = cost_function\n",
    "        self.learning_rate = learning_rate\n",
    "        self.iterations = None\n",
    "        \n",
    "    def train(self, learning_rate=0.01, num_iterations=5000):\n",
    "        self.iterations = []\n",
    "        prior_iteration = None\n",
    "        for iteration in range(num_iterations):\n",
    "            model_iteration = ModelIteration(self, self.data, self.Y, learning_rate, prior_iteration)\n",
    "            self.iterations.append(model_iteration)\n",
    "            \n",
    "            iteration_output = model_iteration.feed_forward(self.data)\n",
    "            model_iteration.propegate_backward() # update weights\n",
    "    \n",
    "            prior_iteration = model_iteration\n",
    "            \n",
    "            if iteration % 500 == 0:\n",
    "                print(\"Completed iteration {}.  Loss: {}\".format(iteration, self.evaluate(self.data, self.Y)))\n",
    "                \n",
    "        return self.evaluate(self.data, self.Y)\n",
    "            \n",
    "    def predict(self, data=None):\n",
    "        self.assert_trained()\n",
    "        if data is None:\n",
    "            data = self.data\n",
    "        return self.iterations[-1].predict(data)\n",
    "    \n",
    "    def evaluate(self, data=None, Y=None):\n",
    "        self.assert_trained()\n",
    "        if data is None:\n",
    "            data = self.data\n",
    "            Y = self.Y\n",
    "        return self.iterations[-1].evaluate(data, Y)\n",
    "    \n",
    "    def assert_trained(self):\n",
    "        if self.iterations is None:\n",
    "            raise Exception(\"Must train before running `predict`.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Layer Version (Complete Version)\n",
    "\n",
    "Now we need to update Layer for backpropegation.  We can start by making an `update_weights` function that executes the core premise: \n",
    "\n",
    "    new_weights = old_weights - learning_rate * derivatives\n",
    "\n",
    "where *derivatives* is the derivative of the loss function with respect to the weights.  This is the step where neural network introductions appear to get complex because of the differential calculus.  Before this, however, let us review why we care about derivatives at all.\n",
    "\n",
    "### Gradient Descent\n",
    "\n",
    "This neural network (specifically `ModelIteration`) started with a random weight initialization.  From there, its goal is to approach a solution that decreases the cost function (log loss) of the model.  The derivative of the cost function with respect to the weight answers the question \"Does increasing this weight increase the cost function?\"\n",
    "\n",
    "Gradient Descent uses this insight and the following equation.  The equation uses a learning rate to slow each step (e.g. typical values may be as small as 0.001).  And by subtracting the product, the gradient descent increases the weight when it *decreases* the cost function.\n",
    "\n",
    "    new_weights = old_weights - learning_rate * derivatives\n",
    "\n",
    "### Estimating Derivatives\n",
    "\n",
    "Better estimators are more involved so we'll simply look at the slope for the line between the points *W-e* and *W+e*, where *W* are the weights and *e* is some small number epsilon.  This is defined as:\n",
    "\n",
    "    slope = (f(W+e) - f(W-e)) / 2e\n",
    "    \n",
    "Here, the function *f* is our neural network feed forward.  We are asking, \"How much does a change of episilon in a weight impact the log loss of our model?\"  `calculate_derivatives` achieves this with two `for` loops and two calls to `Model.evaluate()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class Layer:\n",
    "    def __init__(self, model_iteration, size, activation_function, weights, betas):\n",
    "        self.model_iteration = model_iteration  # locator pattern\n",
    "        self.size = size\n",
    "        self.activation_function = activation_function\n",
    "        self.weights = weights\n",
    "        self.betas = betas\n",
    "        self.derivatives = None\n",
    "\n",
    "    def apply_weights(self, layer_input):\n",
    "        Z = np.dot(self.weights, layer_input) + self.betas\n",
    "        output = self.activation_function(Z)\n",
    "        return output\n",
    "    \n",
    "    def update_weights(self, learning_rate):\n",
    "        if self.derivatives is None:\n",
    "            self.calculate_derivatives()\n",
    "        self.weights = self.weights - learning_rate * self.derivatives\n",
    "\n",
    "    def update_derivatives(self, derivatives):\n",
    "        self.derivatives = derivatives\n",
    "        \n",
    "    def calculate_derivatives(self):\n",
    "        derivatives = np.zeros(self.weights.shape)\n",
    "        \n",
    "        for i in range(self.weights.shape[0]):\n",
    "            for j in range(self.weights.shape[1]):\n",
    "                original_weight = self.weights[i][j]\n",
    "                \n",
    "                self.weights[i][j] = original_weight - DERIVATIVE_OFFSET\n",
    "                cost1 = self.model_iteration.evaluate()\n",
    "                \n",
    "                self.weights[i][j] = original_weight + DERIVATIVE_OFFSET\n",
    "                cost2 = self.model_iteration.evaluate()\n",
    "                \n",
    "                derivative = (cost2 - cost1) / (2 * DERIVATIVE_OFFSET)\n",
    "                \n",
    "                self.weights[i][j] = original_weight\n",
    "                derivatives[i][j] = derivative\n",
    "        return derivatives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activation Functions\n",
    "\n",
    "Our activation functions are generic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def relu_activation(x):\n",
    "    \"\"\"Vectorized relu activation function\n",
    "    :return: 0 if x is less than 0, x otherwise.\n",
    "    \"\"\"\n",
    "    return np.multiply(x, x >= 0)\n",
    "    \n",
    "def sigmoid_activation(x):\n",
    "    \"\"\"Vectorized sigmoid activation function\n",
    "    :return: sigmoid of x\n",
    "    \"\"\"\n",
    "    return 1. / (1 + np.exp(-x))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss Functions\n",
    "\n",
    "Because we used numerical methods to estimate the derivatives of the loss function, we aren't required to use a function whose derivative we know.  However, we'll use the same log loss regardless as it is appropriate to the domain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import log_loss\n",
    "\n",
    "def binary_loss_function(predictions, Y):\n",
    "    \"\"\"Loss function for a binary classifier\"\"\"\n",
    "    return log_loss(Y, predictions[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Putting It Together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed iteration 0.  Loss: 0.6944345809769773\n",
      "Completed iteration 500.  Loss: 0.6926015940033244\n",
      "Completed iteration 1000.  Loss: 0.688595214148807\n",
      "Completed iteration 1500.  Loss: 0.6531413134770567\n",
      "Completed iteration 2000.  Loss: 0.39048408832526843\n",
      "Completed iteration 2500.  Loss: 0.28035440345288426\n",
      "Completed iteration 3000.  Loss: 0.24562189129776962\n",
      "Completed iteration 3500.  Loss: 0.14640653709358822\n",
      "Completed iteration 4000.  Loss: 0.030148034382672922\n",
      "Completed iteration 4500.  Loss: 0.016441491236813478\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.01209920722895147"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn import preprocessing\n",
    "\n",
    "DERIVATIVE_OFFSET = 0.1\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "iris_x = preprocessing.scale(iris[\"data\"])\n",
    "iris_y = iris[\"target\"]\n",
    "Y2 = (iris_y == 0).astype(int)\n",
    "\n",
    "structure = [{\"size\": 5, \"activation\": relu_activation}, \n",
    "             {\"size\": 4, \"activation\": relu_activation}, \n",
    "             {\"size\": 1, \"activation\": sigmoid_activation}]\n",
    "\n",
    "model = Model(iris_x, Y2, structure, binary_loss_function, learning_rate=0.001)\n",
    "model.train(num_iterations=5000)\n",
    "model.evaluate()\n",
    "\n",
    "\n",
    "\n",
    "       \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed iteration 0.  Loss: 0.693649352903605\n",
      "Completed iteration 500.  Loss: 0.3166304798036371\n",
      "Completed iteration 1000.  Loss: 0.12439496934813418\n",
      "Completed iteration 1500.  Loss: 0.06914991084997027\n",
      "Completed iteration 2000.  Loss: 0.04762865231407385\n",
      "Completed iteration 2500.  Loss: 0.03655683018123989\n",
      "Completed iteration 3000.  Loss: 0.02983781658974081\n",
      "Completed iteration 3500.  Loss: 0.025309496352508995\n",
      "Completed iteration 4000.  Loss: 0.022034367490845967\n",
      "Completed iteration 4500.  Loss: 0.019541892510647287\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.017568200408783704"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TRAIN_SIZE = 140\n",
    "\n",
    "iris_train = iris_x[1:TRAIN_SIZE]\n",
    "iris_test = iris_x[TRAIN_SIZE:]\n",
    "Y_train = Y2[1:TRAIN_SIZE]\n",
    "Y_test = Y2[TRAIN_SIZE:]\n",
    "\n",
    "model = Model(iris_train, Y_train, structure, binary_loss_function, learning_rate=0.001)\n",
    "model.train(num_iterations=5000)\n",
    "model.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_hat = (model.predict(iris_test) > 0.5).astype(int)\n",
    "\n",
    "model.predict(data=iris_test) > 0.5\n",
    "Y_test"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
